{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Topics and Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, I will show how to transform documents from one vector representation\n",
        "into another. This process serves two goals:\n",
        "\n",
        "1. To bring out hidden structure in the corpus, discover relationships between\n",
        "   words and use them to describe the documents in a new and\n",
        "   (hopefully) more semantic way.\n",
        "2. To make the document representation more compact. This both improves efficiency\n",
        "   (new representation consumes less resources) and efficacy (marginal data\n",
        "   trends are ignored, noise-reduction).\n",
        "\n",
        "## Creating the Corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n",
        "# remove common words and tokenize\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-29 18:23:44,070 : INFO : collecting document frequencies\n",
            "2023-12-29 18:23:44,071 : INFO : PROGRESS: processing document #0\n",
            "2023-12-29 18:23:44,072 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 9 documents and 12 features (28 matrix non-zeros)', 'datetime': '2023-12-29T18:23:44.072376', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'initialize'}\n"
          ]
        }
      ],
      "source": [
        "# step 1 -- initialize a model\n",
        "\n",
        "from gensim import models\n",
        "\n",
        "tfidf = models.TfidfModel(corpus)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforming vectors\n",
        "\n",
        "- tfidf 被视为只读对象，可用于将任何向量从旧表示（词袋整数计数）转换为新表示（TfIdf 实值权重）\n",
        "- 调用 model[corpus] 仅在旧的 corpus 文档流周围创建一个包装器 - 实际转换是在文档迭代期间即时完成的\n",
        "- 我们无法在调用 corpus_transformed = model[corpus] 时转换整个语料库，因为这意味着将结果存储在主内存中，这与 gensim 内存独立的目标相矛盾\n",
        "- 如果您想多次迭代转换 corpus_transformed (注意转换成本高昂)请首先将生成的语料库序列化到磁盘，然后继续使用它"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
          ]
        }
      ],
      "source": [
        "# step 2 -- use the model to transform vectors\n",
        "doc_bow = [(0, 1), (1, 1)]\n",
        "print(tfidf[doc_bow])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
            "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
            "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
            "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
            "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
            "[(9, 1.0)]\n",
            "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
            "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
            "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
          ]
        }
      ],
      "source": [
        "corpus_tfidf = tfidf[corpus]\n",
        "for doc in corpus_tfidf:\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-29 19:50:13,912 : INFO : using serial LSI version on this node\n",
            "2023-12-29 19:50:13,913 : INFO : updating model with new documents\n",
            "2023-12-29 19:50:13,915 : INFO : preparing a new chunk of documents\n",
            "2023-12-29 19:50:13,917 : INFO : using 100 extra samples and 2 power iterations\n",
            "2023-12-29 19:50:13,918 : INFO : 1st phase: constructing (12, 102) action matrix\n",
            "2023-12-29 19:50:13,922 : INFO : orthonormalizing (12, 102) action matrix\n",
            "2023-12-29 19:50:13,930 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
            "2023-12-29 19:50:13,934 : INFO : computing the final decomposition\n",
            "2023-12-29 19:50:13,936 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
            "2023-12-29 19:50:13,937 : INFO : processed documents up to #9\n",
            "2023-12-29 19:50:13,939 : INFO : topic #0(1.594): -0.703*\"trees\" + -0.538*\"graph\" + -0.402*\"minors\" + -0.187*\"survey\" + -0.061*\"system\" + -0.060*\"time\" + -0.060*\"response\" + -0.058*\"user\" + -0.049*\"computer\" + -0.035*\"interface\"\n",
            "2023-12-29 19:50:13,940 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n",
            "2023-12-29 19:50:13,941 : INFO : LsiModel lifecycle event {'msg': 'trained LsiModel<num_terms=12, num_topics=2, decay=1.0, chunksize=20000> in 0.03s', 'datetime': '2023-12-29T19:50:13.941401', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "# Transformations can also be serialized, one on top of another, in a sort of chain:\n",
        "# 通过潜在语义索引将 Tf-Idf 语料库转换为潜在 2-D 空间(2-D because we set num_topics=2)\n",
        "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
        "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-29 19:50:51,020 : INFO : topic #0(1.594): -0.703*\"trees\" + -0.538*\"graph\" + -0.402*\"minors\" + -0.187*\"survey\" + -0.061*\"system\" + -0.060*\"time\" + -0.060*\"response\" + -0.058*\"user\" + -0.049*\"computer\" + -0.035*\"interface\"\n",
            "2023-12-29 19:50:51,022 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '-0.703*\"trees\" + -0.538*\"graph\" + -0.402*\"minors\" + -0.187*\"survey\" + -0.061*\"system\" + -0.060*\"time\" + -0.060*\"response\" + -0.058*\"user\" + -0.049*\"computer\" + -0.035*\"interface\"'),\n",
              " (1,\n",
              "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# “trees”、“graph” 和 “minors” 都是相关词（并且对第一个主题的方向贡献最大）\n",
        "# 而第二个主题实际上与所有其他词有关\n",
        "# 正如预期的那样，前五个文档与第二个主题的相关性更强，而其余四个文档与第一个主题的相关性更强：\n",
        "lsi_model.print_topics(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, -0.06600783396090276), (1, -0.5200703306361842)] Human machine interface for lab abc computer applications\n",
            "[(0, -0.19667592859142433), (1, -0.7609563167700051)] A survey of user opinion of computer system response time\n",
            "[(0, -0.08992639972446302), (1, -0.7241860626752507)] The EPS user interface management system\n",
            "[(0, -0.07585847652178028), (1, -0.6320551586003427)] System and human system engineering testing of EPS\n",
            "[(0, -0.10150299184980069), (1, -0.573730848300296)] Relation of user perceived response time to error measurement\n",
            "[(0, -0.7032108939378308), (1, 0.1611518021402568)] The generation of random binary unordered trees\n",
            "[(0, -0.8774787673119832), (1, 0.16758906864659276)] The intersection graph of paths in trees\n",
            "[(0, -0.9098624686818582), (1, 0.14086553628718887)] Graph minors IV Widths of trees and well quasi ordering\n",
            "[(0, -0.6165825350569287), (1, -0.05392907566389443)] Graph minors A survey\n"
          ]
        }
      ],
      "source": [
        "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
        "for doc, as_text in zip(corpus_lsi, documents):\n",
        "    print(doc, as_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model persistency is achieved with the :func:`save` and :func:`load` functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 模型持久性是通过 save() 和 load() 函数实现的\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
        "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
        "\n",
        "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
        "\n",
        "os.unlink(tmp.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next question might be: just how exactly similar are those documents to each other?\n",
        "Is there a way to formalize the similarity, so that for a given input document, we can\n",
        "order some other set of documents according to their similarity? Similarity queries\n",
        "are covered in the next tutorial (`sphx_glr_auto_examples_core_run_similarity_queries.py`).\n",
        "\n",
        "\n",
        "## Available transformations\n",
        "\n",
        "Gensim implements several popular Vector Space Model algorithms:\n",
        "\n",
        "* `Term Frequency * Inverse Document Frequency, Tf-Idf <http://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_\n",
        "  expects a bag-of-words (integer values) training corpus during initialization.\n",
        "  During transformation, it will take a vector and return another vector of the\n",
        "  same dimensionality, except that features which were rare in the training corpus\n",
        "  will have their value increased.\n",
        "  It therefore converts integer-valued vectors into real-valued ones, while leaving\n",
        "  the number of dimensions intact. It can also optionally normalize the resulting\n",
        "  vectors to (Euclidean) unit length.\n",
        "\n",
        " .. sourcecode:: pycon\n",
        "\n",
        "    model = models.TfidfModel(corpus, normalize=True)\n",
        "\n",
        "* `Okapi Best Matching, Okapi BM25 <https://en.wikipedia.org/wiki/Okapi_BM25>`_\n",
        "  expects a bag-of-words (integer values) training corpus during initialization.\n",
        "  During transformation, it will take a vector and return another vector of the\n",
        "  same dimensionality, except that features which were rare in the training corpus\n",
        "  will have their value increased. It therefore converts integer-valued\n",
        "  vectors into real-valued ones, while leaving the number of dimensions intact.\n",
        "\n",
        "  Okapi BM25 is the standard ranking function used by search engines to estimate\n",
        "  the relevance of documents to a given search query.\n",
        "\n",
        " .. sourcecode:: pycon\n",
        "\n",
        "    model = models.OkapiBM25Model(corpus)\n",
        "\n",
        "* `Latent Semantic Indexing, LSI (or sometimes LSA) <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_\n",
        "  transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into\n",
        "  a latent space of a lower dimensionality. For the toy corpus above we used only\n",
        "  2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended\n",
        "  as a \"golden standard\" [1]_.\n",
        "\n",
        "  .. sourcecode:: pycon\n",
        "\n",
        "    model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)\n",
        "\n",
        "  LSI training is unique in that we can continue \"training\" at any point, simply\n",
        "  by providing more training documents. This is done by incremental updates to\n",
        "  the underlying model, in a process called `online training`. Because of this feature, the\n",
        "  input document stream may even be infinite -- just keep feeding LSI new documents\n",
        "  as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
        "\n",
        "  .. sourcecode:: pycon\n",
        "\n",
        "    model.add_documents(another_tfidf_corpus)  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
        "    lsi_vec = model[tfidf_vec]  # convert some new document into the LSI space, without affecting the model\n",
        "\n",
        "    model.add_documents(more_documents)  # tfidf_corpus + another_tfidf_corpus + more_documents\n",
        "    lsi_vec = model[tfidf_vec]\n",
        "\n",
        "  See the :mod:`gensim.models.lsimodel` documentation for details on how to make\n",
        "  LSI gradually \"forget\" old observations in infinite streams. If you want to get dirty,\n",
        "  there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical\n",
        "  precision of the LSI algorithm.\n",
        "\n",
        "  `gensim` uses a novel online incremental streamed distributed training algorithm (quite a mouthful!),\n",
        "  which I published in [5]_. `gensim` also executes a stochastic multi-pass algorithm\n",
        "  from Halko et al. [4]_ internally, to accelerate in-core part\n",
        "  of the computations.\n",
        "  See also `wiki` for further speed-ups by distributing the computation across\n",
        "  a cluster of computers.\n",
        "\n",
        "* `Random Projections, RP <http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf>`_ aim to\n",
        "  reduce vector space dimensionality. This is a very efficient (both memory- and\n",
        "  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.\n",
        "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
        "\n",
        "  .. sourcecode:: pycon\n",
        "\n",
        "    model = models.RpModel(tfidf_corpus, num_topics=500)\n",
        "\n",
        "* `Latent Dirichlet Allocation, LDA <http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>`_\n",
        "  is yet another transformation from bag-of-words counts into a topic space of lower\n",
        "  dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA),\n",
        "  so LDA's topics can be interpreted as probability distributions over words. These distributions are,\n",
        "  just like with LSA, inferred automatically from a training corpus. Documents\n",
        "  are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
        "\n",
        "  .. sourcecode:: pycon\n",
        "\n",
        "    model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
        "\n",
        "  `gensim` uses a fast implementation of online LDA parameter estimation based on [2]_,\n",
        "  modified to run in `distributed mode <distributed>` on a cluster of computers.\n",
        "\n",
        "* `Hierarchical Dirichlet Process, HDP <http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>`_\n",
        "  is a non-parametric bayesian method (note the missing number of requested topics):\n",
        "\n",
        "  .. sourcecode:: pycon\n",
        "\n",
        "    model = models.HdpModel(corpus, id2word=dictionary)\n",
        "\n",
        "  `gensim` uses a fast, online implementation based on [3]_.\n",
        "  The HDP model is a new addition to `gensim`, and still rough around its academic edges -- use with care.\n",
        "\n",
        "Adding new :abbr:`VSM (Vector Space Model)` transformations (such as different weighting schemes) is rather trivial;\n",
        "see the `apiref` or directly the `Python code <https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py>`_\n",
        "for more info and examples.\n",
        "\n",
        "It is worth repeating that these are all unique, **incremental** implementations,\n",
        "which do not require the whole training corpus to be present in main memory all at once.\n",
        "With memory taken care of, I am now improving `distributed`,\n",
        "to improve CPU efficiency, too.\n",
        "If you feel you could contribute by testing, providing use-cases or code, see the `Gensim Developer guide <https://github.com/RaRe-Technologies/gensim/wiki/Developer-page>`__.\n",
        "\n",
        "## What Next?\n",
        "\n",
        "Continue on to the next tutorial on `sphx_glr_auto_examples_core_run_similarity_queries.py`.\n",
        "\n",
        "## References\n",
        "\n",
        ".. [1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.\n",
        "\n",
        ".. [2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.\n",
        "\n",
        ".. [3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.\n",
        "\n",
        ".. [4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.\n",
        "\n",
        ".. [5] Řehůřek. 2011. Subspace tracking for Latent Semantic Analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread('run_topics_and_transformations.png')\n",
        "imgplot = plt.imshow(img)\n",
        "_ = plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
