{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Doc2Vec Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Prepare the Training and Test Data\n",
        "\n",
        "- 该语料库包含选自澳大利亚广播公司新闻邮件服务的 314 个文档，该服务提供头条新闻的文本电子邮件并涵盖许多广泛的主题。\n",
        "- And we’ll test our model by eye using the much shorter Lee Corpus which contains 50 documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim\n",
        "# Set file names for train and test data\n",
        "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
        "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a Function to Read and Preprocess Text\n",
        "\n",
        "- To train the model, we'll need to associate a tag/number with each document\n",
        "  of the training corpus. \n",
        "- In our case, the tag is simply the zero-based line\n",
        "  number.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            # 主要用于将文本行进行分词，并进行一些基本的文本清理操作\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                # 用于表示已标记文档的数据结构，通常用于训练 Doc2Vec 模型\n",
        "                # 返回结果： 创建的 TaggedDocument 对象包含了词语列表和标签，用于在训练 Doc2Vec 模型时表示一个已标记的文档\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])     # [i]表示该文档的标签，通常是一个唯一的标识符\n",
        "\n",
        "train_corpus = list(read_corpus(lee_train_file))\n",
        "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
          ]
        }
      ],
      "source": [
        "print(len(train_corpus))\n",
        "print(train_corpus[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
          ]
        }
      ],
      "source": [
        "print(len(test_corpus))\n",
        "print(test_corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model\n",
        "\n",
        "- 实例化一个 Doc2Vec 模型，其向量大小为 50 个维度，并在训练语料库上迭代 40 次。我们将最小字数设置为 2，以便丢弃出现次数很少的单词。\n",
        "- 在实际使用中（一般有数十万到数百万个文档），典型迭代计数为 10-20。\n",
        "- 但这是一个非常非常小的数据集（300 个文档），文档较短（几百个单词）。添加训练迭代次数有时可以帮助处理如此小的数据集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-10 10:18:55,200 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>', 'datetime': '2024-01-10T10:18:55.200962', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<gensim.models.doc2vec.Doc2Vec at 0x1bdc81d50>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(model)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-10 10:19:35,442 : INFO : collecting all words and their counts\n",
            "2024-01-10 10:19:35,443 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
            "2024-01-10 10:19:35,455 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
            "2024-01-10 10:19:35,456 : INFO : Creating a fresh vocabulary\n",
            "2024-01-10 10:19:35,472 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3955 unique words (56.65% of original 6981, drops 3026)', 'datetime': '2024-01-10T10:19:35.472465', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "2024-01-10 10:19:35,473 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 55126 word corpus (94.80% of original 58152, drops 3026)', 'datetime': '2024-01-10T10:19:35.473319', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "2024-01-10 10:19:35,497 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2024-01-10 10:19:35,498 : INFO : sample=0.001 downsamples 46 most-common words\n",
            "2024-01-10 10:19:35,499 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 42390.98914085061 word corpus (76.9%% of prior 55126)', 'datetime': '2024-01-10T10:19:35.499818', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "2024-01-10 10:19:35,539 : INFO : estimated required memory for 3955 words and 50 dimensions: 3679500 bytes\n",
            "2024-01-10 10:19:35,540 : INFO : resetting layer weights\n"
          ]
        }
      ],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 可通过 ``model.wv.index_to_key`` 访问所有唯一单词的列表\n",
        "- 使用 ``model.wv.get_vecattr()`` 方法可以获取每个单词的附加属性(见如下示例)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "40\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(model.corpus_count)\n",
        "print(model.epochs)\n",
        "# 查看 penalty 在训练语料库中出现的次数\n",
        "model.wv.get_vecattr('penalty', 'count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 在通常情况下，需要安装一个用于优化批量向量操作的 BLAS 库，对这个 300 个小文档、约 60k 字语料库的训练应该只需要几秒钟。 \n",
        "- 如不使用 BLAS 库，则需要花费 60 倍到 120 倍的时间\n",
        "- 注：BLAS 库安装使用参见相关文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-10 10:20:29,736 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-01-10T10:20:29.736749', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'train'}\n",
            "2024-01-10 10:20:29,789 : INFO : EPOCH 0: training on 58152 raw words (42747 effective words) took 0.0s, 861707 effective words/s\n",
            "2024-01-10 10:20:29,838 : INFO : EPOCH 1: training on 58152 raw words (42667 effective words) took 0.0s, 930217 effective words/s\n",
            "2024-01-10 10:20:29,885 : INFO : EPOCH 2: training on 58152 raw words (42682 effective words) took 0.0s, 939360 effective words/s\n",
            "2024-01-10 10:20:29,935 : INFO : EPOCH 3: training on 58152 raw words (42637 effective words) took 0.0s, 897420 effective words/s\n",
            "2024-01-10 10:20:29,998 : INFO : EPOCH 4: training on 58152 raw words (42757 effective words) took 0.1s, 709065 effective words/s\n",
            "2024-01-10 10:20:30,051 : INFO : EPOCH 5: training on 58152 raw words (42718 effective words) took 0.0s, 887159 effective words/s\n",
            "2024-01-10 10:20:30,100 : INFO : EPOCH 6: training on 58152 raw words (42664 effective words) took 0.0s, 925680 effective words/s\n",
            "2024-01-10 10:20:30,148 : INFO : EPOCH 7: training on 58152 raw words (42724 effective words) took 0.0s, 938534 effective words/s\n",
            "2024-01-10 10:20:30,195 : INFO : EPOCH 8: training on 58152 raw words (42681 effective words) took 0.0s, 967952 effective words/s\n",
            "2024-01-10 10:20:30,243 : INFO : EPOCH 9: training on 58152 raw words (42625 effective words) took 0.0s, 965539 effective words/s\n",
            "2024-01-10 10:20:30,291 : INFO : EPOCH 10: training on 58152 raw words (42622 effective words) took 0.0s, 945598 effective words/s\n",
            "2024-01-10 10:20:30,338 : INFO : EPOCH 11: training on 58152 raw words (42716 effective words) took 0.0s, 949060 effective words/s\n",
            "2024-01-10 10:20:30,384 : INFO : EPOCH 12: training on 58152 raw words (42567 effective words) took 0.0s, 977494 effective words/s\n",
            "2024-01-10 10:20:30,430 : INFO : EPOCH 13: training on 58152 raw words (42666 effective words) took 0.0s, 975496 effective words/s\n",
            "2024-01-10 10:20:30,480 : INFO : EPOCH 14: training on 58152 raw words (42760 effective words) took 0.0s, 892628 effective words/s\n",
            "2024-01-10 10:20:30,529 : INFO : EPOCH 15: training on 58152 raw words (42589 effective words) took 0.0s, 920281 effective words/s\n",
            "2024-01-10 10:20:30,578 : INFO : EPOCH 16: training on 58152 raw words (42679 effective words) took 0.0s, 914681 effective words/s\n",
            "2024-01-10 10:20:30,629 : INFO : EPOCH 17: training on 58152 raw words (42845 effective words) took 0.0s, 890974 effective words/s\n",
            "2024-01-10 10:20:30,675 : INFO : EPOCH 18: training on 58152 raw words (42664 effective words) took 0.0s, 994022 effective words/s\n",
            "2024-01-10 10:20:30,721 : INFO : EPOCH 19: training on 58152 raw words (42680 effective words) took 0.0s, 981037 effective words/s\n",
            "2024-01-10 10:20:30,802 : INFO : EPOCH 20: training on 58152 raw words (42822 effective words) took 0.1s, 553422 effective words/s\n",
            "2024-01-10 10:20:30,849 : INFO : EPOCH 21: training on 58152 raw words (42687 effective words) took 0.0s, 942746 effective words/s\n",
            "2024-01-10 10:20:30,894 : INFO : EPOCH 22: training on 58152 raw words (42699 effective words) took 0.0s, 998071 effective words/s\n",
            "2024-01-10 10:20:30,941 : INFO : EPOCH 23: training on 58152 raw words (42623 effective words) took 0.0s, 960727 effective words/s\n",
            "2024-01-10 10:20:30,990 : INFO : EPOCH 24: training on 58152 raw words (42691 effective words) took 0.0s, 924188 effective words/s\n",
            "2024-01-10 10:20:31,037 : INFO : EPOCH 25: training on 58152 raw words (42656 effective words) took 0.0s, 949645 effective words/s\n",
            "2024-01-10 10:20:31,083 : INFO : EPOCH 26: training on 58152 raw words (42725 effective words) took 0.0s, 986956 effective words/s\n",
            "2024-01-10 10:20:31,124 : INFO : EPOCH 27: training on 58152 raw words (42698 effective words) took 0.0s, 1114573 effective words/s\n",
            "2024-01-10 10:20:31,162 : INFO : EPOCH 28: training on 58152 raw words (42695 effective words) took 0.0s, 1218458 effective words/s\n",
            "2024-01-10 10:20:31,200 : INFO : EPOCH 29: training on 58152 raw words (42622 effective words) took 0.0s, 1178057 effective words/s\n",
            "2024-01-10 10:20:31,238 : INFO : EPOCH 30: training on 58152 raw words (42745 effective words) took 0.0s, 1195723 effective words/s\n",
            "2024-01-10 10:20:31,274 : INFO : EPOCH 31: training on 58152 raw words (42711 effective words) took 0.0s, 1240014 effective words/s\n",
            "2024-01-10 10:20:31,313 : INFO : EPOCH 32: training on 58152 raw words (42671 effective words) took 0.0s, 1153668 effective words/s\n",
            "2024-01-10 10:20:31,350 : INFO : EPOCH 33: training on 58152 raw words (42702 effective words) took 0.0s, 1205649 effective words/s\n",
            "2024-01-10 10:20:31,387 : INFO : EPOCH 34: training on 58152 raw words (42688 effective words) took 0.0s, 1228075 effective words/s\n",
            "2024-01-10 10:20:31,424 : INFO : EPOCH 35: training on 58152 raw words (42666 effective words) took 0.0s, 1208668 effective words/s\n",
            "2024-01-10 10:20:31,463 : INFO : EPOCH 36: training on 58152 raw words (42692 effective words) took 0.0s, 1143996 effective words/s\n",
            "2024-01-10 10:20:31,505 : INFO : EPOCH 37: training on 58152 raw words (42634 effective words) took 0.0s, 1067581 effective words/s\n",
            "2024-01-10 10:20:31,543 : INFO : EPOCH 38: training on 58152 raw words (42706 effective words) took 0.0s, 1167893 effective words/s\n",
            "2024-01-10 10:20:31,583 : INFO : EPOCH 39: training on 58152 raw words (42694 effective words) took 0.0s, 1168555 effective words/s\n",
            "2024-01-10 10:20:31,584 : INFO : Doc2Vec lifecycle event {'msg': 'training on 2326080 raw words (1707517 effective words) took 1.8s, 924759 effective words/s', 'datetime': '2024-01-10T10:20:31.584045', 'gensim': '4.3.2', 'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]', 'platform': 'macOS-14.1.2-x86_64-i386-64bit', 'event': 'train'}\n"
          ]
        }
      ],
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.19389434 -0.37575087 -0.11150538  0.17023939 -0.19585437 -0.08501209\n",
            " -0.02834594  0.06310242 -0.25008348 -0.17818134  0.16944358 -0.08258659\n",
            "  0.00545225  0.02568397 -0.0194872  -0.05995872  0.15472971  0.16811125\n",
            "  0.06364274 -0.10676058  0.02367179  0.05606707  0.14834112 -0.03242365\n",
            " -0.04256508  0.08342563 -0.23089176  0.07234342 -0.12182193 -0.07851215\n",
            "  0.45746467  0.14566699  0.21811247  0.13564608  0.12847733  0.20201543\n",
            " -0.01480675 -0.2522898  -0.12386173  0.02103999  0.0154345  -0.02884163\n",
            " -0.12639856 -0.12402748  0.02785637  0.0171064  -0.03167505 -0.10304504\n",
            "  0.1724661   0.03696358]\n"
          ]
        }
      ],
      "source": [
        "# 使用模型来推断文本片段的向量\n",
        "# 之后可以通过余弦相似度将该向量与其他向量进行比较\n",
        "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
        "print(vector)\n",
        "# 注意：由于底层训练 / 推理算法是一个利用内部随机化的迭代近似问题，因此同一文本的重复推理将返回略有不同的向量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessing the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0.9700483679771423), (48, 0.8897426724433899), (255, 0.8708517551422119), (40, 0.8607314825057983), (33, 0.8395884037017822), (8, 0.836052656173706), (272, 0.8317200541496277), (264, 0.7745107412338257), (105, 0.7607932686805725), (198, 0.7241007685661316), (19, 0.7184740304946899), (9, 0.7178652286529541), (113, 0.6899672150611877), (25, 0.6892133355140686), (4, 0.6846137046813965), (10, 0.6803380846977234), (84, 0.6456793546676636), (62, 0.6398455500602722), (189, 0.6387141942977905), (46, 0.6315411329269409), (42, 0.5984219908714294), (144, 0.5978928804397583), (188, 0.5962110161781311), (109, 0.5857176184654236), (172, 0.5791205167770386), (2, 0.5784093141555786), (219, 0.5635756850242615), (77, 0.5616114139556885), (43, 0.5585625767707825), (52, 0.5576309561729431), (180, 0.5564634203910828), (21, 0.5552763342857361), (78, 0.5451232194900513), (89, 0.5450719594955444), (126, 0.543415904045105), (256, 0.5416069626808167), (5, 0.5386147499084473), (15, 0.5374650359153748), (222, 0.5361341834068298), (171, 0.5239061117172241), (63, 0.5110018849372864), (136, 0.5066742897033691), (7, 0.5063748955726624), (86, 0.5037515759468079), (123, 0.5020448565483093), (18, 0.5015459060668945), (39, 0.49580878019332886), (79, 0.49469926953315735), (240, 0.4945116341114044), (238, 0.49179574847221375), (290, 0.4915211498737335), (282, 0.4819716811180115), (178, 0.4765635132789612), (202, 0.47519075870513916), (32, 0.47317811846733093), (11, 0.4692029356956482), (124, 0.45970284938812256), (108, 0.4581848978996277), (53, 0.4534681737422943), (181, 0.4508076310157776), (24, 0.44284194707870483), (205, 0.4357629716396332), (70, 0.43506890535354614), (165, 0.43051913380622864), (72, 0.42847710847854614), (185, 0.428338885307312), (27, 0.4278630316257477), (280, 0.42454794049263), (236, 0.42422646284103394), (288, 0.42228686809539795), (289, 0.42110419273376465), (281, 0.4209992587566376), (96, 0.420388787984848), (30, 0.4175233542919159), (298, 0.41678106784820557), (76, 0.4144262969493866), (193, 0.41274186968803406), (22, 0.41070595383644104), (161, 0.40648216009140015), (223, 0.40464669466018677), (285, 0.40173715353012085), (36, 0.40131643414497375), (230, 0.40023496747016907), (154, 0.3999261260032654), (228, 0.3961900472640991), (182, 0.39502716064453125), (265, 0.39289772510528564), (296, 0.39154985547065735), (286, 0.38524484634399414), (163, 0.38367733359336853), (137, 0.3832749128341675), (69, 0.38109129667282104), (83, 0.37952250242233276), (239, 0.37784233689308167), (149, 0.37487873435020447), (291, 0.3704672157764435), (47, 0.36766690015792847), (210, 0.3657345175743103), (169, 0.36259007453918457), (190, 0.35935279726982117), (99, 0.35747072100639343), (102, 0.3563964068889618), (142, 0.35324037075042725), (248, 0.35217997431755066), (91, 0.35062506794929504), (164, 0.3475733995437622), (139, 0.347273051738739), (225, 0.3462635576725006), (16, 0.3458063006401062), (208, 0.34373393654823303), (263, 0.34365788102149963), (141, 0.3424995243549347), (120, 0.34004679322242737), (242, 0.3392501175403595), (191, 0.3390909433364868), (59, 0.3360801935195923), (175, 0.33576181530952454), (179, 0.3355836868286133), (117, 0.33319106698036194), (234, 0.3313199579715729), (162, 0.328626811504364), (271, 0.32829204201698303), (196, 0.3230910897254944), (253, 0.321979284286499), (13, 0.3216021656990051), (129, 0.3211438059806824), (212, 0.31930112838745117), (204, 0.3163115978240967), (284, 0.31609416007995605), (1, 0.3149615526199341), (95, 0.3147158920764923), (92, 0.314376562833786), (237, 0.3089708685874939), (14, 0.3083617091178894), (50, 0.307143896818161), (203, 0.3047913908958435), (87, 0.30412375926971436), (127, 0.30268046259880066), (100, 0.3018015921115875), (98, 0.3017391562461853), (260, 0.3011031150817871), (133, 0.30050522089004517), (150, 0.2997526526451111), (73, 0.2990432381629944), (58, 0.2989967465400696), (66, 0.2989954650402069), (209, 0.2964576780796051), (71, 0.2949092388153076), (51, 0.2927570939064026), (254, 0.2865176796913147), (183, 0.2851392328739166), (251, 0.28366589546203613), (156, 0.2833583950996399), (218, 0.28082364797592163), (168, 0.28065383434295654), (207, 0.2805616557598114), (266, 0.27907201647758484), (31, 0.27276596426963806), (277, 0.27230340242385864), (67, 0.26818108558654785), (121, 0.26812657713890076), (243, 0.26601162552833557), (211, 0.2598728835582733), (65, 0.25968098640441895), (112, 0.25845885276794434), (276, 0.25600340962409973), (224, 0.25544267892837524), (119, 0.2549110949039459), (104, 0.25422438979148865), (82, 0.2540363669395447), (213, 0.2522290349006653), (151, 0.24824635684490204), (166, 0.24497674405574799), (55, 0.24488580226898193), (45, 0.24447016417980194), (49, 0.24442271888256073), (106, 0.24397972226142883), (294, 0.24093320965766907), (115, 0.24053874611854553), (54, 0.2401951551437378), (101, 0.23905052244663239), (167, 0.23671847581863403), (159, 0.23566316068172455), (278, 0.23396562039852142), (258, 0.233693927526474), (227, 0.2323174923658371), (122, 0.23196297883987427), (35, 0.23145177960395813), (155, 0.23105356097221375), (229, 0.2274390459060669), (226, 0.22662323713302612), (177, 0.22540538012981415), (268, 0.2252436876296997), (186, 0.22394683957099915), (34, 0.22254370152950287), (81, 0.2224833071231842), (215, 0.22136525809764862), (135, 0.22096604108810425), (152, 0.22076460719108582), (261, 0.22024109959602356), (44, 0.21437235176563263), (187, 0.21435587108135223), (64, 0.21425336599349976), (143, 0.21276724338531494), (252, 0.2124544084072113), (128, 0.21136218309402466), (114, 0.20865800976753235), (295, 0.20792652666568756), (270, 0.20557311177253723), (192, 0.20472334325313568), (107, 0.20405681431293488), (37, 0.20334044098854065), (297, 0.20301353931427002), (88, 0.2029360830783844), (287, 0.19981758296489716), (68, 0.19358858466148376), (292, 0.1934836059808731), (174, 0.1910618394613266), (26, 0.1910420060157776), (249, 0.19091640412807465), (61, 0.18953019380569458), (257, 0.18891330063343048), (217, 0.18624114990234375), (220, 0.18488483130931854), (206, 0.18216298520565033), (145, 0.18097056448459625), (28, 0.18021829426288605), (275, 0.1782688945531845), (146, 0.1779496818780899), (195, 0.17568103969097137), (273, 0.17558060586452484), (200, 0.17116113007068634), (20, 0.169589102268219), (94, 0.16681817173957825), (90, 0.1656295210123062), (244, 0.16526944935321808), (199, 0.16043689846992493), (299, 0.15891893208026886), (293, 0.15690289437770844), (269, 0.15344585478305817), (274, 0.14725086092948914), (246, 0.1444113552570343), (233, 0.1419028341770172), (74, 0.1410570591688156), (125, 0.14001967012882233), (80, 0.13818754255771637), (148, 0.1352936327457428), (201, 0.1337241679430008), (93, 0.13335858285427094), (279, 0.13025081157684326), (221, 0.12925945222377777), (118, 0.12427794933319092), (56, 0.12372362613677979), (245, 0.12294481694698334), (57, 0.12227068841457367), (38, 0.12139137089252472), (85, 0.1198071539402008), (247, 0.11847780644893646), (6, 0.11719496548175812), (147, 0.11662383377552032), (231, 0.1156965047121048), (216, 0.11503585427999496), (138, 0.11446099728345871), (176, 0.11215495318174362), (259, 0.1115521639585495), (132, 0.11046271026134491), (250, 0.11019634455442429), (140, 0.10627081990242004), (110, 0.10250242799520493), (262, 0.10237406939268112), (130, 0.10153982043266296), (75, 0.09992715716362), (17, 0.09635885804891586), (111, 0.09454341232776642), (194, 0.08789585530757904), (241, 0.0848938375711441), (267, 0.08174560964107513), (153, 0.08116399496793747), (41, 0.08114636689424515), (97, 0.08072064071893692), (160, 0.08053112030029297), (232, 0.07938041538000107), (184, 0.06642475724220276), (158, 0.06608467549085617), (170, 0.06240185350179672), (103, 0.06032515689730644), (12, 0.04599703848361969), (173, 0.04598332569003105), (116, 0.04417642205953598), (29, 0.0434303916990757), (60, 0.029497619718313217), (131, 0.023323077708482742), (235, 0.010560959577560425), (23, 0.0015352688496932387), (134, -0.0005593550740741193), (3, -0.02334580384194851), (283, -0.04065607860684395), (214, -0.06672781705856323), (157, -0.07287175953388214), (197, -0.0875849723815918)]\n",
            "0\n",
            "========\n",
            "[(1, 0.9504474401473999), (143, 0.7149805426597595), (141, 0.7081637382507324), (223, 0.699680745601654), (217, 0.6897391080856323), (51, 0.6603817939758301), (85, 0.6600404381752014), (34, 0.6385636329650879), (242, 0.629584550857544), (208, 0.6282011270523071), (86, 0.6250258684158325), (110, 0.6192390322685242), (277, 0.619150698184967), (81, 0.60124272108078), (169, 0.5989917516708374), (75, 0.5949415564537048), (125, 0.5919942855834961), (226, 0.5915729999542236), (14, 0.5882635116577148), (177, 0.5864582657814026), (259, 0.5863209366798401), (122, 0.5808083415031433), (76, 0.5807928442955017), (231, 0.5782544016838074), (261, 0.5653148293495178), (234, 0.5638390183448792), (268, 0.5617563724517822), (243, 0.5591951012611389), (13, 0.5530981421470642), (220, 0.5477086901664734), (93, 0.5473626852035522), (292, 0.5442761778831482), (197, 0.542375922203064), (173, 0.5403311848640442), (249, 0.537481963634491), (11, 0.5367286205291748), (160, 0.5332263112068176), (201, 0.532586932182312), (227, 0.5253819227218628), (133, 0.5247114896774292), (233, 0.5228753685951233), (12, 0.5181446671485901), (267, 0.516903281211853), (66, 0.5129321217536926), (240, 0.5125170946121216), (62, 0.5079060196876526), (63, 0.5044732093811035), (131, 0.5036799907684326), (116, 0.5009717345237732), (159, 0.4996150732040405), (290, 0.4988767206668854), (44, 0.49765563011169434), (174, 0.49708986282348633), (145, 0.49612921476364136), (210, 0.49386897683143616), (200, 0.4924798011779785), (50, 0.47761106491088867), (58, 0.47578391432762146), (82, 0.4750387966632843), (21, 0.4677498936653137), (286, 0.45786505937576294), (192, 0.4563846290111542), (41, 0.45427370071411133), (235, 0.4509649872779846), (183, 0.450231671333313), (2, 0.4498211741447449), (26, 0.44899359345436096), (269, 0.44765353202819824), (5, 0.44554251432418823), (94, 0.44164231419563293), (148, 0.4412641227245331), (54, 0.430418461561203), (74, 0.43038031458854675), (57, 0.42765161395072937), (167, 0.4248376786708832), (176, 0.424548864364624), (166, 0.41843733191490173), (250, 0.41450345516204834), (295, 0.40844500064849854), (152, 0.4080638885498047), (29, 0.40635421872138977), (186, 0.40601417422294617), (198, 0.404911071062088), (32, 0.4049035608768463), (127, 0.40103477239608765), (18, 0.39884501695632935), (153, 0.3961985111236572), (60, 0.38867026567459106), (216, 0.3886476457118988), (276, 0.3870040774345398), (24, 0.3867057263851166), (88, 0.3852885067462921), (77, 0.38103461265563965), (184, 0.38064971566200256), (225, 0.3786000609397888), (23, 0.37184590101242065), (35, 0.3692626953125), (98, 0.36902815103530884), (45, 0.3613336980342865), (9, 0.35917341709136963), (89, 0.35824498534202576), (100, 0.3551103472709656), (296, 0.35188987851142883), (258, 0.35139864683151245), (39, 0.34974223375320435), (257, 0.34728699922561646), (134, 0.341697096824646), (188, 0.3398377597332001), (114, 0.3374486565589905), (113, 0.33443164825439453), (56, 0.33368563652038574), (284, 0.32841968536376953), (43, 0.32692673802375793), (211, 0.32341405749320984), (28, 0.3216789960861206), (163, 0.3188275098800659), (219, 0.31838899850845337), (3, 0.31811144948005676), (178, 0.31594446301460266), (49, 0.31585514545440674), (107, 0.3142217695713043), (164, 0.31405943632125854), (130, 0.31393948197364807), (256, 0.3115295469760895), (126, 0.3048778772354126), (91, 0.30361396074295044), (209, 0.30187809467315674), (0, 0.3009849488735199), (10, 0.30040985345840454), (79, 0.2909660041332245), (15, 0.28986695408821106), (255, 0.28689688444137573), (65, 0.28354591131210327), (266, 0.2819664180278778), (293, 0.27780622243881226), (97, 0.275992214679718), (248, 0.2757083773612976), (283, 0.27395108342170715), (147, 0.270309180021286), (84, 0.2698608636856079), (142, 0.2679588496685028), (196, 0.2640012204647064), (253, 0.26044443249702454), (121, 0.2593410909175873), (72, 0.25922611355781555), (46, 0.25848135352134705), (124, 0.25532853603363037), (236, 0.25523874163627625), (239, 0.25271159410476685), (281, 0.2520477771759033), (71, 0.25194692611694336), (205, 0.25137874484062195), (189, 0.24963794648647308), (224, 0.2494829148054123), (80, 0.24709780514240265), (288, 0.24551557004451752), (170, 0.24541279673576355), (218, 0.23993337154388428), (140, 0.2398122251033783), (230, 0.23971866071224213), (69, 0.23817959427833557), (280, 0.23144273459911346), (207, 0.22716470062732697), (36, 0.22458989918231964), (25, 0.22431184351444244), (7, 0.2234276384115219), (48, 0.21897399425506592), (132, 0.2177601009607315), (298, 0.21453188359737396), (59, 0.21443027257919312), (22, 0.21102099120616913), (297, 0.21092066168785095), (181, 0.2086593508720398), (193, 0.20661012828350067), (212, 0.20235371589660645), (27, 0.20173698663711548), (68, 0.2001737505197525), (138, 0.19721567630767822), (263, 0.19692429900169373), (67, 0.1954999715089798), (136, 0.1942078322172165), (206, 0.19300784170627594), (271, 0.1914924681186676), (40, 0.19034022092819214), (73, 0.18764452636241913), (162, 0.18296374380588531), (282, 0.18293294310569763), (33, 0.18077902495861053), (222, 0.17882075905799866), (37, 0.17867521941661835), (182, 0.17713642120361328), (115, 0.1765383630990982), (123, 0.17640800774097443), (8, 0.1747608780860901), (202, 0.17088986933231354), (171, 0.16981108486652374), (96, 0.16861063241958618), (119, 0.1685851812362671), (279, 0.16774284839630127), (285, 0.16627436876296997), (105, 0.16576792299747467), (108, 0.16423307359218597), (16, 0.15398244559764862), (180, 0.153830423951149), (191, 0.1534709632396698), (109, 0.15059255063533783), (264, 0.14855659008026123), (214, 0.146529883146286), (272, 0.14254650473594666), (185, 0.14240697026252747), (289, 0.14166975021362305), (95, 0.1410662680864334), (228, 0.13597355782985687), (154, 0.13361911475658417), (244, 0.12829330563545227), (157, 0.12794837355613708), (117, 0.12761589884757996), (265, 0.12666814029216766), (135, 0.12655285000801086), (120, 0.1204759031534195), (70, 0.11943253129720688), (144, 0.11788085848093033), (47, 0.11691365391016006), (139, 0.11677374690771103), (199, 0.11440277844667435), (20, 0.1130228117108345), (118, 0.11177383363246918), (42, 0.11060431599617004), (294, 0.11058253794908524), (237, 0.10753826797008514), (252, 0.10710199922323227), (99, 0.1042795181274414), (19, 0.10385192930698395), (129, 0.09859932959079742), (61, 0.09857909381389618), (128, 0.09689294546842575), (53, 0.09433022886514664), (102, 0.09268569201231003), (275, 0.09114065766334534), (106, 0.08781995624303818), (274, 0.08424578607082367), (238, 0.08292538672685623), (78, 0.0811200812458992), (204, 0.07864759117364883), (213, 0.07229597121477127), (161, 0.06890305131673813), (291, 0.06537728756666183), (90, 0.0633520632982254), (270, 0.060478296130895615), (155, 0.058902665972709656), (245, 0.054424040019512177), (278, 0.05245402827858925), (112, 0.052391648292541504), (260, 0.05203430354595184), (179, 0.04996170476078987), (137, 0.049734484404325485), (87, 0.04481863975524902), (187, 0.04117879644036293), (92, 0.040241602808237076), (104, 0.03355151414871216), (195, 0.03318494185805321), (83, 0.029875844717025757), (4, 0.029255865141749382), (229, 0.027135834097862244), (221, 0.026291316375136375), (262, 0.025798646733164787), (246, 0.025235531851649284), (30, 0.023863017559051514), (215, 0.02329432964324951), (146, 0.022184375673532486), (273, 0.020678285509347916), (172, 0.017955368384718895), (203, 0.01697218045592308), (175, 0.016182666644454002), (158, 0.013509472832083702), (151, 0.011508606374263763), (52, 0.008482703007757664), (194, 0.003928051330149174), (101, -0.006822254974395037), (111, -0.014286036603152752), (38, -0.014595973305404186), (247, -0.028695637360215187), (103, -0.028842363506555557), (64, -0.0365242063999176), (55, -0.03943272680044174), (254, -0.04374167323112488), (150, -0.04485674947500229), (156, -0.06091596558690071), (165, -0.06257423758506775), (149, -0.06924274563789368), (17, -0.07251527160406113), (287, -0.07456903159618378), (6, -0.07754781097173691), (168, -0.0789242759346962), (299, -0.08134158700704575), (190, -0.08514659851789474), (251, -0.09264535456895828), (232, -0.10183902084827423), (241, -0.11225052177906036), (31, -0.14706525206565857)]\n",
            "0\n",
            "========\n",
            "[(2, 0.9213272929191589), (21, 0.8094497919082642), (18, 0.7709668278694153), (43, 0.7695923447608948), (205, 0.7475721836090088), (123, 0.7310607433319092), (39, 0.7114983797073364), (7, 0.6959573030471802), (196, 0.6915287375450134), (32, 0.6873928904533386), (46, 0.6852133274078369), (86, 0.6828881502151489), (136, 0.6764270067214966), (193, 0.67350172996521), (189, 0.6699099540710449), (178, 0.6670774817466736), (248, 0.6659464836120605), (163, 0.6637946367263794), (171, 0.6611195206642151), (279, 0.6551895141601562), (286, 0.6531952023506165), (256, 0.6504294872283936), (71, 0.6504200100898743), (240, 0.6480952501296997), (15, 0.6476935744285583), (108, 0.6433544158935547), (11, 0.6390630602836609), (144, 0.6356560587882996), (77, 0.634185791015625), (121, 0.6328693628311157), (296, 0.6262060403823853), (266, 0.6236240267753601), (27, 0.6085767149925232), (255, 0.6079322695732117), (169, 0.6019930839538574), (242, 0.5996388792991638), (24, 0.5902310013771057), (207, 0.5861148238182068), (14, 0.5856696367263794), (188, 0.5852410197257996), (91, 0.5850808620452881), (223, 0.5817471146583557), (258, 0.5804554224014282), (76, 0.578793466091156), (66, 0.5719238519668579), (54, 0.5702269673347473), (25, 0.568472683429718), (298, 0.5663175582885742), (113, 0.5652433633804321), (67, 0.5646316409111023), (95, 0.5595877766609192), (62, 0.556416392326355), (9, 0.5544057488441467), (1, 0.5539128184318542), (72, 0.5500153303146362), (181, 0.5466198921203613), (10, 0.5461143255233765), (272, 0.5458123683929443), (68, 0.5445139408111572), (63, 0.5439904928207397), (47, 0.5435842871665955), (290, 0.5362182855606079), (281, 0.5343586206436157), (0, 0.5320319533348083), (124, 0.5309268832206726), (208, 0.5299420952796936), (22, 0.5293044447898865), (48, 0.5236063599586487), (125, 0.5235647559165955), (179, 0.522118091583252), (261, 0.5219982266426086), (59, 0.5204582810401917), (288, 0.5169392228126526), (30, 0.5165374279022217), (174, 0.5153239369392395), (244, 0.5152999758720398), (280, 0.5122932195663452), (210, 0.5077838897705078), (154, 0.5069059133529663), (231, 0.5046742558479309), (269, 0.4999709129333496), (33, 0.49523842334747314), (42, 0.4923355281352997), (100, 0.49158939719200134), (291, 0.4895130395889282), (170, 0.4844323694705963), (51, 0.48405811190605164), (198, 0.4825955331325531), (202, 0.4791065454483032), (282, 0.4790073037147522), (87, 0.477611243724823), (89, 0.4760417342185974), (99, 0.47603505849838257), (224, 0.4744900166988373), (260, 0.4738180935382843), (94, 0.47344380617141724), (217, 0.4728999733924866), (277, 0.46878138184547424), (186, 0.46606338024139404), (40, 0.4618579149246216), (8, 0.4591606557369232), (172, 0.4587731957435608), (264, 0.45691558718681335), (52, 0.45537644624710083), (132, 0.4551505744457245), (137, 0.4550026059150696), (101, 0.45308586955070496), (34, 0.4520591199398041), (180, 0.45143672823905945), (45, 0.4508824050426483), (265, 0.4479513168334961), (84, 0.44755762815475464), (149, 0.4469926953315735), (164, 0.44373202323913574), (295, 0.4434310495853424), (139, 0.4405296742916107), (141, 0.4402257800102234), (80, 0.4400753974914551), (5, 0.437724232673645), (289, 0.4374758303165436), (177, 0.4336542785167694), (122, 0.4329117238521576), (215, 0.43212011456489563), (4, 0.42996835708618164), (105, 0.42752668261528015), (135, 0.4274526536464691), (221, 0.42634278535842896), (117, 0.4238974153995514), (138, 0.4226549565792084), (239, 0.42160147428512573), (175, 0.41979900002479553), (234, 0.4183618128299713), (85, 0.4170990586280823), (236, 0.41393157839775085), (230, 0.4086538851261139), (3, 0.40849190950393677), (206, 0.40847331285476685), (278, 0.40452030301094055), (57, 0.4021311104297638), (120, 0.4020368456840515), (237, 0.3978438675403595), (16, 0.39433160424232483), (262, 0.39346522092819214), (203, 0.3907652199268341), (253, 0.3885255455970764), (259, 0.3882088363170624), (128, 0.3880787789821625), (199, 0.38705959916114807), (110, 0.38672685623168945), (182, 0.3854074478149414), (96, 0.3851749897003174), (287, 0.3850259780883789), (150, 0.3827931880950928), (285, 0.3827785551548004), (238, 0.38265907764434814), (225, 0.3809020519256592), (74, 0.38074830174446106), (187, 0.37946784496307373), (116, 0.3788159191608429), (226, 0.3766687214374542), (168, 0.37340813875198364), (294, 0.37317678332328796), (275, 0.3714243173599243), (247, 0.37125685811042786), (56, 0.3710801303386688), (201, 0.3697584569454193), (36, 0.3688080310821533), (156, 0.36785033345222473), (191, 0.3674830198287964), (197, 0.36620426177978516), (118, 0.36400800943374634), (143, 0.3613036274909973), (23, 0.3587234318256378), (161, 0.3583493232727051), (82, 0.3554619252681732), (185, 0.35495179891586304), (37, 0.3541673421859741), (145, 0.3534807860851288), (222, 0.35169216990470886), (61, 0.34696394205093384), (102, 0.3460615873336792), (227, 0.33632537722587585), (129, 0.330328106880188), (251, 0.3302745521068573), (93, 0.3291431665420532), (166, 0.32552266120910645), (173, 0.32545965909957886), (273, 0.3248123526573181), (218, 0.3242340087890625), (70, 0.3220716714859009), (276, 0.32061922550201416), (167, 0.3178713321685791), (219, 0.3176761269569397), (257, 0.31585758924484253), (220, 0.31419119238853455), (228, 0.31364840269088745), (13, 0.3129420578479767), (204, 0.30928415060043335), (44, 0.30867883563041687), (263, 0.3049063980579376), (60, 0.3048996031284332), (243, 0.30358535051345825), (31, 0.30244338512420654), (17, 0.3022719621658325), (190, 0.3006059229373932), (160, 0.30054929852485657), (130, 0.29901206493377686), (211, 0.29786041378974915), (26, 0.29712384939193726), (271, 0.296107679605484), (241, 0.2943434417247772), (232, 0.2937406599521637), (73, 0.29366159439086914), (233, 0.2927895486354828), (148, 0.28588753938674927), (20, 0.28558409214019775), (81, 0.28408437967300415), (53, 0.2836001515388489), (249, 0.2834152281284332), (19, 0.2826281785964966), (246, 0.2825712561607361), (79, 0.28149378299713135), (195, 0.2799626588821411), (267, 0.2751157283782959), (245, 0.27476993203163147), (146, 0.2737264335155487), (75, 0.27306610345840454), (194, 0.2720795273780823), (78, 0.26871466636657715), (126, 0.26727861166000366), (69, 0.26433151960372925), (152, 0.2630334198474884), (109, 0.25849106907844543), (131, 0.24946576356887817), (274, 0.24919840693473816), (55, 0.24812892079353333), (142, 0.24518872797489166), (165, 0.24510830640792847), (112, 0.24388344585895538), (270, 0.24257345497608185), (104, 0.24200491607189178), (103, 0.24127565324306488), (212, 0.23815187811851501), (111, 0.23498086631298065), (155, 0.2347871959209442), (292, 0.23423776030540466), (158, 0.2300420105457306), (6, 0.22958599030971527), (92, 0.22937999665737152), (229, 0.22788283228874207), (293, 0.22784242033958435), (49, 0.22489629685878754), (283, 0.22290602326393127), (88, 0.2199542224407196), (252, 0.2159954160451889), (28, 0.2119334191083908), (254, 0.20359095931053162), (209, 0.20334741473197937), (157, 0.20193971693515778), (297, 0.19578412175178528), (213, 0.19303672015666962), (151, 0.18995621800422668), (29, 0.183085635304451), (83, 0.18246322870254517), (162, 0.1818152219057083), (147, 0.18090426921844482), (159, 0.177603617310524), (235, 0.17578819394111633), (268, 0.1704300194978714), (65, 0.16625992953777313), (184, 0.1654491424560547), (106, 0.16386090219020844), (50, 0.15984784066677094), (250, 0.15878012776374817), (90, 0.155827134847641), (214, 0.1477288007736206), (38, 0.14460772275924683), (299, 0.13741962611675262), (153, 0.13163349032402039), (98, 0.11507523059844971), (41, 0.11421341449022293), (183, 0.1139373704791069), (58, 0.10943020135164261), (97, 0.10911808162927628), (119, 0.1061854362487793), (133, 0.10485599935054779), (115, 0.10483839362859726), (134, 0.09156416356563568), (192, 0.09091049432754517), (176, 0.09019321203231812), (64, 0.08676157146692276), (127, 0.08077715337276459), (200, 0.0681876614689827), (12, 0.0557682029902935), (284, 0.0442928671836853), (35, 0.042518604546785355), (216, 0.037541523575782776), (140, 0.036023858934640884), (107, 0.002868508454412222), (114, -0.08655264228582382)]\n",
            "0\n",
            "========\n"
          ]
        }
      ],
      "source": [
        "# 测试项\n",
        "for doc_id in range(3):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    print(sims)\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    print(rank)\n",
        "    print(\"========\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 首先为训练语料库的每个文档推断新向量，将推断的向量与训练语料库进行比较，然后根据自相似性返回文档的排名\n",
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)     # 返回第一次出现该文档标识符的位置\n",
        "    ranks.append(rank)\n",
        "\n",
        "    # 跟踪第二个排名，以比较不太相似的文档\n",
        "    second_ranks.append(sims[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "300\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]),\n",
              " TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(model.dv))\n",
        "print(len(train_corpus))\n",
        "train_corpus[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(48, 0.8941465020179749),\n",
              " (143, 0.7156313061714172),\n",
              " (21, 0.8416993618011475),\n",
              " (57, 0.71437668800354),\n",
              " (33, 0.7626050114631653)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "second_ranks[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 正常应该本文档是自己的相似度最高，但也不完全是\n",
        "# 例:有多少个相似度最高的不是本文档的\n",
        "[rank for rank in ranks if rank!=0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({0: 293, 1: 7})\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "# 每个文档在训练语料库中的排名\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 基本上，超过 95% 的推断文档被发现与自身最相似，大约 5% 的情况下它被错误地与另一个文档最相似。\n",
        "- 根据训练向量检查推断向量是一种 “健全性检查”，以确定模型是否以有效一致的方式运行，尽管不是真正的 “准确性” 值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
            "\n",
            "MOST (299, 0.947761595249176): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
            "\n",
            "SECOND-MOST (104, 0.7992818355560303): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»\n",
            "\n",
            "MEDIAN (199, 0.24182011187076569): «the royal commission into the building industry has ended the first day of public hearings in melbourne counsel assisting lionel robbards qc told the commission of culture of fear in the building industry he said some witnesses were afraid to come forward after being physically threatened the construction forestry mining and energy union cfmeu secretary martin kingham will respond to allegations made against the unions when he gives evidence tomorrow it coincides with rally by thousands of building workers outside collins place where the commission is being held»\n",
            "\n",
            "LEAST (243, -0.11476718634366989): «four afghan factions have reached agreement on an interim cabinet during talks in germany the united nations says the administration which will take over from december will be headed by the royalist anti taliban commander hamed karzai it concludes more than week of negotiations outside bonn and is aimed at restoring peace and stability to the war ravaged country the year old former deputy foreign minister who is currently battling the taliban around the southern city of kandahar is an ally of the exiled afghan king mohammed zahir shah he will serve as chairman of an interim authority that will govern afghanistan for six month period before loya jirga or grand traditional assembly of elders in turn appoints an month transitional government meanwhile united states marines are now reported to have been deployed in eastern afghanistan where opposition forces are closing in on al qaeda soldiers reports from the area say there has been gun battle between the opposition and al qaeda close to the tora bora cave complex where osama bin laden is thought to be hiding in the south of the country american marines are taking part in patrols around the air base they have secured near kandahar but are unlikely to take part in any assault on the city however the chairman of the joint chiefs of staff general richard myers says they are prepared for anything they are prepared for engagements they re robust fighting force and they re absolutely ready to engage if that required he said»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 示例1\n",
        "# doc_id: 299\n",
        "# sims: 最后一个的most_similar列表\n",
        "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Notice above that the most similar document (usually the same text) is has a similarity score approaching 1.0. \n",
        "- However, the similarity score for the second-ranked documents should be significantly lower (assuming the documents are in fact different) \n",
        "- and the reasoning becomes obvious when we examine the text itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Document (76): «the death toll in argentina food riots has risen to local media reports say four more people died this morning in clashes between police and protesters near the presidential palace in the capital buenos aires president fernando de la rua has called on the opposition to take part in government of national unity and apparently will resign if it does not looting and rioting has generally given way to more peaceful demonstrations against the faltering government blamed for month recession heavily armed police using powers under day state of siege decree are attempting to prevent large public gatherings but union leaders say workers and the unemployed will not stop until the government is removed and living standards restored with argentina discredited economy minister now gone the government hopes to approve new budget acceptable to the international monetary fund imf to avoid default on the billion foreign debt the presidents of neighbouring brazil and chile say they fear the social unrest could infect their own nations unless argentina and its leaders can resolve the crisis quickly»\n",
            "\n",
            "Similar Document (66, 0.7584123611450195): «argentina government has crumbled after at least people were killed and hundreds injured in nationwide riots argentina president fernando de la rua has resigned and called for national unity government with the opposition peronists the president resignation followed hours of rioting across the country people took to the streets protesting against the government economic austerity program argentina is now on the brink of defaulting on its next debt repayment which could be the largest default ever the opposition parties are reported to have rejected the call for national unity government in washington the international monetary fund said it would work with the new cabinet government policy has previously ruled out any devaluation of the peso fearing run on the currency and an even greater debt crisis the government has also declared day state of siege in an effort to restore order»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 示例2\n",
        "# Pick a random document from the corpus and infer a vector from the model\n",
        "import random\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "# Compare and print the second-most-similar document\n",
        "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "sim_id = second_ranks[doc_id]\n",
        "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the Model\n",
        "\n",
        "Using the same approach above, we'll infer the vector for a randomly chosen\n",
        "test document, and compare the document to our model by eye.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Document (39): «the real level of world inequality and environmental degradation may be far worse than official estimates according to leaked document prepared for the world richest countries and seen by the guardian it includes new estimates that the world lost almost of its forests in the past years that carbon dioxide emissions leading to global warming are expected to rise by in rich countries and in the rest of the world in the next years and that more than more fresh water will be needed by»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
            "\n",
            "MOST (171, 0.6015363931655884): «drug education campaigns appear to be paying dividends with new figures showing per cent drop in drug related deaths last year according to the australian bureau of statistics people died from drug related causes in the year that figure is substantial drop from when australians died of drug related causes across the states and territories new south wales recorded the biggest decrease the bureau david payne attributes the decline of drug deaths to the heroin drought in some parts of the country better equipped ambulances and emergency wards and above all effective federal and state drug education campaigns they have put lot of money into the program there has been fall and while you can discern trend from that the figures are going in the right way right direction mr payne said»\n",
            "\n",
            "MEDIAN (284, 0.21473123133182526): «secretary of state colin powell says the united states believes saudi fugitive osama bin laden is in the southern and eastern part of afghanistan mr powell says it is just matter of time before bin laden is found and the ruling taliban defeated appearing on the cbs face the nation program mr powell reported on the status of the us operation in afghanistan the taliban still hanging on in kandahar and some of the southern provinces and the mountains to the east and to the south but they are under enormous pressure he said it is just matter of time before we achieve our objectives mr powell says us president george bush does not care how long the campaign takes he wants osama bin laden he wants al qaeda ripped up and the taliban has to be totally removed from power mr powell said you can be sure that we are looking for bin laden and we have quite few ideas to pursue the united states has total of to troops on the ground in afghanistan according to defense secretary donald rumsfeld we have got to people he said officials had earlier announced that over us marines are deployed near the southern city of kandahar the last stronghold of afghanistan ousted taliban militia the us forces also include light infantry troops in the north mr rumsfeld also confirmed non us coalition forces are also on the ground us officers in afghanistan say british german and australian liaison officers are working with the us marines in southern afghanistan»\n",
            "\n",
            "LEAST (283, -0.2822982668876648): «france is celebrating victory over australia in the davis cup tennis final after nicholas escude defeated wayne arthurs in four sets in the deciding rubber yesterday pat rafter was forced to withdraw from the match with recurring arm injury just hours before he was to take the court the deciding rubber was to be rafter farewell match before taking an indefinite break from the sport arthurs who has not played singles match since october says he was bitterly disappointed to lose any tennis players dream to be out there in front of partisan australain crowd you couldn ask for anything more in your tennis career and just unbelievably disappointed didn get the job done he said team captain john fitzgerald has defended the decision to play rafter in the doubles on saturday saying he was always in doubt for the final match there was no guarantee he could have played tell you anyone with less character wouldn have lasted nearly as long there was no guarantee he could play the doubles to start with let alone the singles and if he had day off there was still no guarantee rafter failed to show up to the post match media conference team officals said he was tired however todd woodbridge hit out at the media for wanting to ask him about his future plans it been asked times every day for two and half weeks mean what do you want him to say he asked reporters french team captain guy forget says the victory in part makes up for france defeat against australia in nice two years ago you re back in australia against better team on grass against the number one player in the world and you pull off win it very very exciting and the way it happened is just very very special he said escude says winning the match for france is the highlight of his career the palestinian authority has launched crackdown on islamic militants arresting more than members of the islamic jihad and hamas groups in the wake of weekend of deadly bomb attacks against israel the israeli government is warning of harsh response to the attacks the latest in the coastal city of haifa lone suicide bomber boarded bus in the northern israeli town and detonated device that killed israeli passengers some of them blown clear of the wreckage that attack came just hours after triple bombing in jerusalem which killed young israelis several israeli government ministers suggested that yasser arafat and the palestinian leadership should now be toppled decision on military response is likely to be made later today when israeli prime minister ariel sharon returns from the united states after the white house demanded action the palestinian authority has declared state of emergency and launched late night raids to arrest scores of hamas and islamic jihad members in the west bank and gaza strip»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "# Compare and print the most/median/least similar documents from the train corpus\n",
        "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
