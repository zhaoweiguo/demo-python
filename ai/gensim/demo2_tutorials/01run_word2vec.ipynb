{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: gensim in /Users/zhaoweiguo/9tool/miniconda3/envs/chatgpt/lib/python3.10/site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /Users/zhaoweiguo/9tool/miniconda3/envs/chatgpt/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /Users/zhaoweiguo/9tool/miniconda3/envs/chatgpt/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/chatgpt/lib/python3.10/site-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Word2Vec Model\n",
        "==============\n",
        "\n",
        "#### 说明\n",
        "\n",
        "- Total running time of the script: ( 11 minutes 26.674 seconds)\n",
        "- Estimated memory usage: 7177 MB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 注意\n",
        "该模型大约为 2GB，因此您需要良好的网络连接才能继续"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "# 加载预训练的 Word2Vec 模型\n",
        "# 这个模型是使用大规模的 Google News 文本数据进行训练的，其中包含了丰富的语义信息\n",
        "wv = api.load('word2vec-google-news-300')   # 最终的本质是: 先下载，然后执行 KeyedVectors.load_word2vec_format(path, binary=False)\n",
        "wv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 模拟输出\n",
        "```\n",
        "<gensim.models.keyedvectors.KeyedVectors at 0x7f257f225ea0>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wv.index_to_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 模拟输出\n",
        "```\n",
        "['</s>',\n",
        " 'in',\n",
        " 'for',\n",
        " 'that',\n",
        " ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 常见操作: 检索模型的词汇表\n",
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 模拟输出\n",
        "```\n",
        "word #0/3000000 is </s>\n",
        "word #1/3000000 is in\n",
        "word #2/3000000 is for\n",
        "word #3/3000000 is that\n",
        "word #4/3000000 is is\n",
        "word #5/3000000 is on\n",
        "word #6/3000000 is ##\n",
        "word #7/3000000 is The\n",
        "word #8/3000000 is with\n",
        "word #9/3000000 is said\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 模型的术语的向量\n",
        "vec_king = wv['king']   # 获取单词 \"king\" 的向量表示\n",
        "vec_king"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 模拟输出\n",
        "```\n",
        "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
        "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
        "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
        "        ... ...\n",
        "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
        "      dtype=float32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vec_king.shape   # (300,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 该模型无法推断不熟悉的单词的向量, 这是 Word2Vec 的一个限制\n",
        "# 如果此限制对您很重要，请查看 FastText 模型\n",
        "try:\n",
        "    vec_cameroon = wv['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Word2Vec 支持多个开箱即用的单词相似性任务\n",
        "# 您可以看到随着单词越来越不相似，相似度如何直观地降低\n",
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 模拟输出\n",
        "```\n",
        "'car'\t'minivan'\t0.69\n",
        "'car'\t'bicycle'\t0.54\n",
        "'car'\t'airplane'\t0.42\n",
        "'car'\t'cereal'\t0.14\n",
        "'car'\t'communism'\t0.06\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 写出 5 个与 “car” 或 “minivan” 最相似的单词\n",
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))\n",
        "# 输出:\n",
        "# [('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 哪项不属于该序列\n",
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))\n",
        "# 输出\n",
        "# car"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Your Own Model\n",
        "-----------------------\n",
        "\n",
        "- 使用 Lee 评估语料库（如果您安装了 Gensim，则已经拥有该语料库）\n",
        "- 这个语料库足够小，可以完全放入内存中\n",
        "- 但我们将实现一个内存友好的迭代器，它可以逐行读取它，以演示如何处理更大的语料库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/zhaoweiguo/9tool/miniconda3/envs/chatgpt/lib/python3.10/site-packages/gensim/test/test_data/lee_background.cor'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.test.utils import datapath\n",
        "datapath('lee_background.cor')\n",
        "# linux输出\n",
        "# /usr/local/lib/python3.10/dist-packages/gensim/test/test_data/lee_background.cor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)\n",
        "print(\"==========================\")\n",
        "print(model)       # <gensim.models.word2vec.Word2Vec at 0x7f257f2267a0>\n",
        "print(model.wv)    # <gensim.models.keyedvectors.KeyedVectors at 0x7f25ac59ed10>   和上面的wv一样"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.02174288,  0.06308484,  0.01441088,  0.01603901,  0.01316035,\n",
              "       -0.11271596,  0.04764793,  0.11911392, -0.00727676, -0.02178196,\n",
              "       -0.00759635, -0.07258018,  0.00744819,  0.03911889,  0.00879618,\n",
              "        0.01496025, -0.00348121, -0.00139711, -0.02534691, -0.08332758,\n",
              "        0.04878865,  0.01100308,  0.01575529, -0.00158456, -0.02796627,\n",
              "        0.0264222 , -0.02473329, -0.01437838, -0.03901039,  0.01756936,\n",
              "        0.04521987, -0.05508058,  0.04803083, -0.04291599, -0.01334529,\n",
              "        0.06593297,  0.02051216,  0.01240187, -0.02542784, -0.04047228,\n",
              "       -0.01836985,  0.00515179, -0.01495974,  0.01715535,  0.03786959,\n",
              "       -0.02594631, -0.03560946, -0.00415782,  0.011525  ,  0.04263784,\n",
              "        0.02222399, -0.03364095, -0.02248785,  0.00086933, -0.02121293,\n",
              "        0.02388584, -0.00050786,  0.00049492, -0.03180176,  0.00444909,\n",
              "       -0.01861753,  0.0002589 ,  0.01320993, -0.00435303, -0.03973737,\n",
              "        0.08123787,  0.02082963,  0.04370801, -0.05350183,  0.06430358,\n",
              "       -0.01294753,  0.00771934,  0.06665884, -0.01006461,  0.05110599,\n",
              "        0.03811992, -0.0012763 , -0.0280916 , -0.05339983, -0.01781596,\n",
              "       -0.04401606,  0.01015398, -0.04354998,  0.05225862, -0.00309495,\n",
              "       -0.0171895 ,  0.02843733,  0.04309303,  0.06513948,  0.01629348,\n",
              "        0.05036229,  0.0710261 ,  0.06048794,  0.0161844 ,  0.12305639,\n",
              "        0.02828362,  0.06155071, -0.00780892,  0.00838462, -0.00748087],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_king = model.wv['king'] #  “wv” stands for “word vectors”\n",
        "vec_king.shape    # (300,)\n",
        "vec_king"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/1750 is the\n",
            "word #1/1750 is to\n",
            "word #2/1750 is of\n",
            "word #3/1750 is in\n",
            "word #4/1750 is and\n",
            "word #5/1750 is he\n",
            "word #6/1750 is is\n",
            "word #7/1750 is for\n",
            "word #8/1750 is on\n",
            "word #9/1750 is said\n"
          ]
        }
      ],
      "source": [
        "# 和上面一样的检索词汇表操作\n",
        "for index, word in enumerate(model.wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing and loading models\n",
        "--------------------------\n",
        "\n",
        "- 使用标准 gensim 方法存储 / 加载模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    #\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc.\n",
        "    #\n",
        "    # To load a saved model:\n",
        "    #\n",
        "    # uses pickle internally\n",
        "    new_model = gensim.models.Word2Vec.load(temporary_filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 加载由原始 C 工具创建的模型(格式说明，不能运行)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load models created by the original C tool, both using its text and binary formats:\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
        "# using gzipped/bz2 input works too, no need to unzip\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Parameters\n",
        "-------------------\n",
        "\n",
        "- ``Word2Vec`` accepts several parameters that affect both training speed and quality.\n",
        "- ``Word2Vec`` 是用于训练 Word Embedding 模型的函数\n",
        "\n",
        "#### min_count\n",
        "\n",
        "- 用于修剪内部字典。在十亿个单词的语料库中只出现一两次的单词可能是无趣的拼写错误和垃圾。此外，没有足够的数据来对这些单词进行任何有意义的训练，因此最好忽略它们：\n",
        "- default value of min_count=5\n",
        "- A reasonable value for min_count is between 0-100, depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec(sentences, min_count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### vector_size\n",
        "\n",
        "- gensim Word2Vec 将单词映射到的 N-dimensional 的维数 (N)\n",
        "- 较大的尺寸值需要更多的训练数据，但可以产生更好（更准确）的模型\n",
        "- 合理的值在数十到数百之间。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The default value of vector_size is 100.\n",
        "model = gensim.models.Word2Vec(sentences, vector_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### workers\n",
        "\n",
        "- 用于训练并行化，以加快训练速度\n",
        "- workers 参数仅在安装了 `Cython <http://cython.org/>`_ 时才有效。如果没有 Cython，如使用 GIL(GlobalInterpreterLock)，您将只能使用一个核心"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# default value of workers=3 (tutorial says 1...)\n",
        "model = gensim.models.Word2Vec(sentences, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory\n",
        "------\n",
        "\n",
        "- word2vec 模型参数存储为矩阵（NumPy 数组）\n",
        "- 每个数组都是 #vocabulary(唯一单词数)乘以浮点数（单精度，即 4 个字节）的向量大小（ vector_size 参数）\n",
        "- #vocabulary(唯一单词数)可以通过 min_count 参数控制\n",
        "- RAM 中保存了三个这样的矩阵（正在努力将该数字减少到两个，甚至一个）\n",
        "- 因此，如果您的输入包含 100,000 个唯一单词，并且您要求层 vector_size=200 ，则模型将需要大约\n",
        "```\n",
        "100,000*200*4*3 bytes = ~229MB\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating\n",
        "----------\n",
        "\n",
        "- Word2Vec 训练是一项无监督任务，没有好的方法来客观评估结果。评估取决于您的最终应用。\n",
        "- 谷歌已经发布了大约 20,000 个句法和语义测试示例的测试集，遵循 “A is to B as C is to D” 任务。它在 “数据集” 文件夹中提供。\n",
        "- 比较类型的句法类比是 ``bad:worse;good:?`` 。数据集中共有 9 种句法比较，如复数名词和相反含义的名词。\n",
        "- 语义问题包含五种类型的语义类比，例如首都 ( Paris:France;Tokyo:? ) 或家庭成员 ( brother:sister;dad:? )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "questions-words.txt 文件中可能包含的一些语法和语义比较类型的示例：\n",
        "```\n",
        "名词的复数形式：\n",
        "    例如：boy boys, girl girls\n",
        "\n",
        "名词的对应形式：\n",
        "    例如：go going, walk walking\n",
        "\n",
        "国家与其首都：\n",
        "    例如：France Paris, Japan Tokyo\n",
        "\n",
        "形容词的比较级：\n",
        "    例如：good better, bad worse\n",
        "\n",
        "名词的所有格：\n",
        "    例如：man's men, woman's women\n",
        "\n",
        "名词与其相关的形容词：\n",
        "    例如：fish fishy, water watery\n",
        "\n",
        "动词的时态：\n",
        "    例如：run running, eat eating\n",
        "\n",
        "相似关系：\n",
        "    例如：cat kitten, dog puppy\n",
        "\n",
        "反义词：\n",
        "    例如：happy sad, hot cold\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gensim supports the same evaluation set, in exactly the same format:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 用于评估模型在词汇类比任务上的性能\n",
        "# questions-words.txt 是一个包含词汇类比问题的文本文件，通常用于测试模型在理解词汇关系方面的能力\n",
        "# 模型会尝试回答类似 “a 与 b 相同，c 与 d 相同，那么 a 与 c 的关系是什么” 的问题\n",
        "# 方法返回一个包含评估结果的字典，其中包括准确率等信息\n",
        "model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 2016年12月发布的 Gensim 中，我们添加了一种更好的方法来评估语义相似性\n",
        "- 默认情况下，它使用学术数据集 WS-353，但您可以基于它创建特定于您的业务的数据集\n",
        "- 它包含单词对以及人工指定的相似性判断。它衡量两个单词的相关性或共现性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 用于评估模型在词汇相似性任务上的性能\n",
        "# wordsim353.tsv 是一个包含词汇相似性评估数据的文件，通常包括人工标注的词对相似度\n",
        "# \n",
        "model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 注意\n",
        "在 Google 或 WS-353 测试集上的良好性能并不意味着 word2vec 在您的应用程序中运行良好，反之亦然。最好直接评估您的预期任务。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Online training / Resuming training\n",
        "-----------------------------------\n",
        "\n",
        "Advanced users can load a model and continue training it with more sentences\n",
        "and `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
        "more_sentences = [\n",
        "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
        "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
        "]\n",
        "model.build_vocab(more_sentences, update=True)\n",
        "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# cleaning up temporary file\n",
        "import os\n",
        "os.remove(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loss Computation\n",
        "-------------------------\n",
        "\n",
        "- 参数 compute_loss 可用于在训练 Word2Vec 模型时进行损失计算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# instantiating and training the Word2Vec model\n",
        "model_with_loss = gensim.models.Word2Vec(\n",
        "    sentences,          # 输入的句子列表\n",
        "    min_count=1,        # 忽略出现次数小于1的词汇\n",
        "    compute_loss=True,  # 计算训练过程中的损失值\n",
        "    hs=0,               # 使用负采样而不是层级softmax\n",
        "    sg=1,               # 使用skip-gram模型\n",
        "    seed=42,            # 随机数种子，以确保可复现性\n",
        ")\n",
        "\n",
        "# getting the training loss value\n",
        "training_loss = model_with_loss.get_latest_training_loss()\n",
        "print(training_loss)    # 1357486.875\n",
        "# 损失值是衡量模型在训练期间学习效果的指标，通常表示模型预测与实际值之间的差异程度。\n",
        "# 在 Word2Vec 模型中，损失值越低表示模型在学习词嵌入时拟合训练数据得越好。\n",
        "# 损失值是模型性能的一个指标，但不应孤立地看待。结合其他评估指标、任务的具体要求以及可能的过拟合情况来全面判断模型的训练效果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks\n",
        "----------\n",
        "\n",
        "我们将使用以下数据进行基准测试：\n",
        "```\n",
        "1. Lee Background corpus: included in gensim’s test data\n",
        "2. Text8 corpus. To demonstrate the effect of corpus size, we’ll look at the first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "import gensim.models.word2vec\n",
        "import gensim.downloader as api\n",
        "import smart_open\n",
        "\n",
        "\n",
        "def head(path, size):\n",
        "    with smart_open.open(path) as fin:\n",
        "        return io.StringIO(fin.read(size))\n",
        "\n",
        "\n",
        "def generate_input_data():\n",
        "    lee_path = datapath('lee_background.cor')\n",
        "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
        "    ls.name = '25kB'\n",
        "    yield ls\n",
        "\n",
        "    text8_path = api.load('text8').fn\n",
        "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
        "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
        "    for l, s in zip(labels, sizes):\n",
        "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
        "        ls.name = l\n",
        "        yield ls\n",
        "\n",
        "\n",
        "input_data = list(generate_input_data())\n",
        "# 输出:\n",
        "# [<gensim.models.word2vec.LineSentence at 0x7f257f4bf250>,\n",
        "#  <gensim.models.word2vec.LineSentence at 0x7f248dce6f80>,\n",
        "#  <gensim.models.word2vec.LineSentence at 0x7f248dce7940>,\n",
        "#  <gensim.models.word2vec.LineSentence at 0x7f248dce5180>,\n",
        "#  <gensim.models.word2vec.LineSentence at 0x7f248dce6680>]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 比较输入数据和模型训练参数（例如 hs 和 sg ）的不同组合所需的训练时间\n",
        "- 对于每个组合，我们重复测试几次以获得测试持续时间的平均值和标准差"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Temporarily reduce logging verbosity\n",
        "logging.root.level = logging.ERROR\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_time_values = []\n",
        "seed_val = 42\n",
        "sg_values = [0, 1]\n",
        "hs_values = [0, 1]\n",
        "\n",
        "fast = True\n",
        "if fast:\n",
        "    input_data_subset = input_data[:3]\n",
        "else:\n",
        "    input_data_subset = input_data\n",
        "\n",
        "\n",
        "for data in input_data_subset:\n",
        "    for sg_val in sg_values:\n",
        "        for hs_val in hs_values:\n",
        "            for loss_flag in [True, False]:\n",
        "                time_taken_list = []\n",
        "                for i in range(3):\n",
        "                    start_time = time.time()\n",
        "                    w2v_model = gensim.models.Word2Vec(\n",
        "                        data,\n",
        "                        compute_loss=loss_flag,\n",
        "                        sg=sg_val,\n",
        "                        hs=hs_val,\n",
        "                        seed=seed_val,\n",
        "                    )\n",
        "                    time_taken_list.append(time.time() - start_time)\n",
        "\n",
        "                time_taken_list = np.array(time_taken_list)\n",
        "                time_mean = np.mean(time_taken_list)\n",
        "                time_std = np.std(time_taken_list)\n",
        "\n",
        "                model_result = {\n",
        "                    'train_data': data.name,\n",
        "                    'compute_loss': loss_flag,\n",
        "                    'sg': sg_val,\n",
        "                    'hs': hs_val,\n",
        "                    'train_time_mean': time_mean,\n",
        "                    'train_time_std': time_std,\n",
        "                }\n",
        "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
        "                train_time_values.append(model_result)\n",
        "\n",
        "train_times_table = pd.DataFrame(train_time_values)\n",
        "train_times_table = train_times_table.sort_values(\n",
        "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
        "    ascending=[False, False, True, False],\n",
        ")\n",
        "print(train_times_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 模拟输出\n",
        "```\n",
        "    compute_loss  hs  sg train_data  train_time_mean  train_time_std\n",
        "4           True   0   1       25kB         0.472116        0.015137\n",
        "5          False   0   1       25kB         0.469522        0.003345\n",
        "6           True   1   1       25kB         0.950259        0.005153\n",
        "7          False   1   1       25kB         0.942416        0.009776\n",
        "0           True   0   0       25kB         0.252174        0.020227\n",
        "1          False   0   0       25kB         0.258985        0.026276\n",
        "2           True   1   0       25kB         0.419408        0.002198\n",
        "3          False   1   0       25kB         0.430876        0.001000\n",
        "12          True   0   1        1MB         1.506507        0.036966\n",
        "13         False   0   1        1MB         1.537814        0.010207\n",
        "14          True   1   1        1MB         3.302257        0.045232\n",
        "15         False   1   1        1MB         3.492871        0.193276\n",
        "8           True   0   0        1MB         0.644114        0.009346\n",
        "9          False   0   0        1MB         0.656217        0.027036\n",
        "10          True   1   0        1MB         1.315072        0.094572\n",
        "11         False   1   0        1MB         1.205833        0.005159\n",
        "20          True   0   1       10MB        20.357308        0.412410\n",
        "21         False   0   1       10MB        21.380845        1.690947\n",
        "22          True   1   1       10MB        44.487718        1.131427\n",
        "23         False   1   1       10MB        44.517535        1.447279\n",
        "16          True   0   0       10MB         7.446084        0.789432\n",
        "17         False   0   0       10MB         7.060013        0.213669\n",
        "18          True   1   0       10MB        14.277136        0.744163\n",
        "19         False   1   0       10MB        13.758649        0.373940\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualising Word Embeddings\n",
        "---------------------------\n",
        "\n",
        "- 可视化可用于注意到数据中的语义和句法趋势:\n",
        "```\n",
        "* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n",
        "* Syntactic: words like run, running or cut, cutting lie close together.\n",
        "```\n",
        "\n",
        "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
        "\n",
        "#### 注意\n",
        ".. Important::\n",
        "  The model used for the visualisation is trained on a small corpus. Thus\n",
        "  some of the relations might not be so clear.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "\n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    # extract the words & their vectors, as numpy arrays\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "\n",
        "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "\n",
        "    #\n",
        "    # Label randomly subsampled 25 data points\n",
        "    #\n",
        "    indices = list(range(len(labels)))\n",
        "    selected_indices = random.sample(indices, 25)\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
        "\n",
        "try:\n",
        "    get_ipython()\n",
        "except Exception:\n",
        "    plot_function = plot_with_matplotlib\n",
        "else:\n",
        "    plot_function = plot_with_plotly\n",
        "\n",
        "plot_function(x_vals, y_vals, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<img src=\"https://img.zhaoweiguo.com/uPic/2024/01/QK9i3z.png\" width=\"30%\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.23801143,  0.7230952 ,  0.17190379, ..., -0.11578286,\n",
              "         0.081784  , -0.08193262],\n",
              "       [-0.28703138,  0.82561386,  0.20189007, ..., -0.11783595,\n",
              "         0.0756435 , -0.09346155],\n",
              "       [-0.2604952 ,  0.7762954 ,  0.16527511, ..., -0.11252926,\n",
              "         0.0964959 , -0.10583613],\n",
              "       ...,\n",
              "       [-0.03199477,  0.07584364,  0.02609465, ..., -0.00832574,\n",
              "        -0.00141254, -0.00856047],\n",
              "       [-0.01083308,  0.0550551 ,  0.02093291, ..., -0.01879125,\n",
              "         0.00293344, -0.00378424],\n",
              "       [-0.03690437,  0.10409687,  0.02872949, ..., -0.01226656,\n",
              "         0.00522907, -0.01340222]], dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.23801143,  0.7230952 ,  0.17190379, ..., -0.11578286,\n",
              "         0.081784  , -0.08193262],\n",
              "       [-0.28703138,  0.82561386,  0.20189007, ..., -0.11783595,\n",
              "         0.0756435 , -0.09346155],\n",
              "       [-0.2604952 ,  0.7762954 ,  0.16527511, ..., -0.11252926,\n",
              "         0.0964959 , -0.10583613],\n",
              "       ...,\n",
              "       [-0.03199477,  0.07584364,  0.02609465, ..., -0.00832574,\n",
              "        -0.00141254, -0.00856047],\n",
              "       [-0.01083308,  0.0550551 ,  0.02093291, ..., -0.01879125,\n",
              "         0.00293344, -0.00378424],\n",
              "       [-0.03690437,  0.10409687,  0.02872949, ..., -0.01226656,\n",
              "         0.00522907, -0.01340222]], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "vectors_raw = np.asarray(model.wv.vectors)\n",
        "vectors_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the', 'to', 'of', 'in', 'and', 'he', 'is', 'for', 'on', 'said']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.index_to_key[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1750\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['the', 'to', 'of', ..., 'finally', 'separate', 'owen'],\n",
              "      dtype='<U14')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = np.asarray(model.wv.index_to_key)\n",
        "print(len(labels))\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1750\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 71.41756  ,  -6.135798 ],\n",
              "       [ 74.32616  ,  -1.0158837],\n",
              "       [ 72.787445 ,  -2.61733  ],\n",
              "       ...,\n",
              "       [-45.747684 , -13.382935 ],\n",
              "       [-57.922096 ,  -5.313386 ],\n",
              "       [-11.694438 ,  18.641695 ]], dtype=float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "num_dimensions=2\n",
        "tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "vectors = tsne.fit_transform(vectors_raw)\n",
        "print(len(vectors))\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
