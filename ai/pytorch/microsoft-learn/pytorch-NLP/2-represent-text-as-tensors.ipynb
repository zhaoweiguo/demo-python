{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Representing text as Tensors](#toc1_)    \n",
    "  - [Text classification task](#toc1_1_)    \n",
    "  - [Tokenization and Vectorization](#toc1_2_)    \n",
    "  - [Bag of Words text representation](#toc1_3_)    \n",
    "  - [Training BoW classifier](#toc1_4_)    \n",
    "  - [BiGrams, TriGrams and N-Grams](#toc1_5_)    \n",
    "  - [Term Frequency Inverse Document Frequency TF-IDF](#toc1_6_)    \n",
    "  - [Bag of Words text representation2](#toc1_7_)    \n",
    "  - [BiGrams, TriGrams and N-Grams2](#toc1_8_)    \n",
    "  - [Term Frequency Inverse Document Frequency TF-IDF2](#toc1_9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Representing text as Tensors](#toc0_)\n",
    "\n",
    "> **NOTE** 如果有一点乱的话，可能是因为，有些内容是从Github中提取出来，有些是官网上提取出来 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gensim==3.8.3 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 1)) (3.8.3)\n",
      "Requirement already satisfied: huggingface==0.0.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 2)) (0.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: nltk==3.5 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: numpy==1.18.5 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 5)) (1.18.5)\n",
      "Requirement already satisfied: opencv-python==4.5.1.48 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 6)) (4.5.1.48)\n",
      "Requirement already satisfied: Pillow==7.1.2 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 7)) (7.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 9)) (1.10.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: torchaudio==0.8.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 11)) (0.8.1)\n",
      "Requirement already satisfied: torchinfo==0.0.8 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 12)) (0.0.8)\n",
      "Requirement already satisfied: torchtext==0.9.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 13)) (0.9.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 14)) (0.9.1)\n",
      "Requirement already satisfied: transformers==4.3.3 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from -r ./requirements.txt (line 15)) (4.3.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from gensim==3.8.3->-r ./requirements.txt (line 1)) (6.4.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from gensim==3.8.3->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: tqdm in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from nltk==3.5->-r ./requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: regex in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from nltk==3.5->-r ./requirements.txt (line 4)) (2023.5.5)\n",
      "Requirement already satisfied: click in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from nltk==3.5->-r ./requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from nltk==3.5->-r ./requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from scikit-learn->-r ./requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from torch==1.8.1->-r ./requirements.txt (line 10)) (4.9.0)\n",
      "Requirement already satisfied: requests in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from torchtext==0.9.1->-r ./requirements.txt (line 13)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from transformers==4.3.3->-r ./requirements.txt (line 15)) (0.10.3)\n",
      "Requirement already satisfied: packaging in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from transformers==4.3.3->-r ./requirements.txt (line 15)) (23.1)\n",
      "Requirement already satisfied: sacremoses in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from transformers==4.3.3->-r ./requirements.txt (line 15)) (0.1.1)\n",
      "Requirement already satisfied: filelock in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from transformers==4.3.3->-r ./requirements.txt (line 15)) (3.12.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ./requirements.txt (line 13)) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ./requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ./requirements.txt (line 13)) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ./requirements.txt (line 13)) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Text classification task](#toc0_)\n",
    "\n",
    "In this module, we will start with a simple text classification task based on **AG_NEWS** dataset, which is to classify news headlines into one of 4 categories: World, Sports, Business and Sci/Tech. This dataset is built into [`torchtext`](https://github.com/pytorch/text) module, so we can easily access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test.csv: 1.86MB [00:00, 6.71MB/s]                          \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(range(5), train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Tokenization and Vectorization](#toc0_)\n",
    "Now we need to convert text into **numbers** that can be represented as tensors. If we want word-level representation, we need to do two things:\n",
    "* use **tokenizer** to split text into **tokens**\n",
    "* build a **vocabulary** of those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torchtext 内置分词器\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建所有标记的词汇表\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 225963), ('the', 203833), (',', 165675), ('to', 119203), ('a', 110149)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(counter.__len__())\n",
    "counter.most_common()[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[283, 2321, 5, 337, 19, 1301, 2357]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using vocabulary, we can easily encode out tokenized string into a set of numbers:\n",
    "# torchtext vocab.stoi 字典允许我们从字符串表示形式转换为数字（名称 stoi 代表“从字符串到整数”）\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Bag of Words text representation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 3.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# To compute bag-of-words vector from the vector representation of our AG_NEWS dataset, we can use the following function:\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(to_bow(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Here we are using global `vocab_size` variable to specify default size of the vocabulary. Since often vocabulary size is pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering `vocab_size` value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance.\n",
    "\n",
    "## <a id='toc1_4_'></a>[Training BoW classifier](#toc0_)\n",
    "\n",
    "- train a classifier on top of BoW. \n",
    "First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. \n",
    "This can be achieved by passing `bowify` function as `collate_fn` parameter to standard torch `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a simple classifier neural network that contains one linear layer. \n",
    "# The size of the input vector equals to `vocab_size`, and output size corresponds to the number of classes (4). \n",
    "# Because we are solving classification task, the final activation function is `LogSoftmax()`.\n",
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size, 4), torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will define standard PyTorch training loop. Because our dataset is quite large, for our teaching purpose we will train only for one epoch, \n",
    "#   and sometimes even for less than an epoch (specifying the `epoch_size` parameter allows us to limit training). \n",
    "# We would also report accumulated training accuracy during training; the frequency of reporting is specified using `report_freq` parameter.\n",
    "def train_epoch(net, dataloader, lr=0.01, optimizer=None, loss_fn = torch.nn.NLLLoss(), epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels, features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out, labels) # cross_entropy(out,labels)\n",
    "        loss.backward()     # 反向传播\n",
    "        optimizer.step()    # 更新参数\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8\n",
      "6400: acc=0.83984375\n",
      "9600: acc=0.8551041666666667\n",
      "12800: acc=0.861875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025586477983226653, 0.8661380597014925)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net, train_loader, epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[BiGrams, TriGrams and N-Grams](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'love',\n",
       " 'to',\n",
       " 'play',\n",
       " 'with',\n",
       " 'my',\n",
       " 'words',\n",
       " 'i love',\n",
       " 'love to',\n",
       " 'to play',\n",
       " 'play with',\n",
       " 'with my',\n",
       " 'my words']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchtext.data.utils import ngrams_iterator\n",
    "line = \"I love to play with my words\"\n",
    "ite = ngrams_iterator(tokenizer(line), ngrams=2)\n",
    "list(ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocab size = 481947\n"
     ]
    }
   ],
   "source": [
    "# 从我们的新闻数据集中构建二元组词汇表\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "bi_counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    bi_counter.update(ngrams_iterator(tokenizer(line),ngrams=2))    # ngrams_iterator 将标记序列转换为 n-gram 序列的函数\n",
    "bi_vocab = torchtext.vocab.Vocab(bi_counter, min_freq=2)\n",
    "\n",
    "print(f\"Bigram vocab size = {len(bi_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7fd811a15760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('first', 9035),\n",
       " ('two', 8926),\n",
       " ('he', 8901),\n",
       " ('for the', 8819),\n",
       " ('world', 8464)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(bi_counter.__len__())\n",
    "bi_counter.most_common()[55:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_6_'></a>[Term Frequency Inverse Document Frequency TF-IDF](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "df = torch.zeros(vocab_size)\n",
    "for _,line in train_dataset[:N]:\n",
    "    for i in set(encode(line)):\n",
    "        df[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0544,  ..., 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "def tf_idf(s):\n",
    "    bow = to_bow(s)     # 术语频率向量\n",
    "    return bow*torch.log((N+1)/(df+1))\n",
    "\n",
    "print(tf_idf(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Bag of Words text representation2](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to generate a bag of word representation using the Scikit Learn python library:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()     # array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[BiGrams, TriGrams and N-Grams2](#toc0_)\n",
    "\n",
    "Below is an example of how to generate a bigram bag of word representation using the Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary length =  1308844\n"
     ]
    }
   ],
   "source": [
    "# The main drawback of N-gram approach is that vocabulary size starts to grow extremely fast. \n",
    "# In practice, we need to combine N-gram representation with some dimensionality reduction techniques, such as *embeddings*, which we will discuss in the next unit.\n",
    "# To use N-gram representation in our **AG News** dataset, we need to build special ngram vocabulary:\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
    "    \n",
    "bi_vocab = torchtext.vocab.Vocab(counter, min_freq=1)\n",
    "\n",
    "print(\"Bigram vocabulary length = \",len(bi_vocab))\n",
    "\n",
    "# We could then use the same code as above to train the classifier, however, it would be very memory-inefficient. \n",
    "# In the next unit, we will train bigram classifier using embeddings.\n",
    "# > **Note:** You can only leave those ngrams that occur in the text more than specified number of times. \n",
    "#   This will make sure that infrequent bigrams will be omitted, and will decrease the dimensionality significantly. \n",
    "#   To do this, set `min_freq` parameter to a higher value, and observe the length of vocabulary change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Term Frequency Inverse Document Frequency TF-IDF2](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF vectorization of text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-py38_pytorch-py"
  },
  "kernelspec": {
   "display_name": "openCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
