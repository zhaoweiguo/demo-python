{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Building the model layers](#toc1_)    \n",
    "  - [Build a neural network](#toc1_1_)    \n",
    "    - [Get a hardware device for training](#toc1_1_1_)    \n",
    "    - [Define the class](#toc1_1_2_)    \n",
    "  - [Weight and Bias](#toc1_2_)    \n",
    "  - [Model layers](#toc1_3_)    \n",
    "    - [nn.Flatten](#toc1_3_1_)    \n",
    "    - [nn.Linear](#toc1_3_2_)    \n",
    "    - [nn.ReLU](#toc1_3_3_)    \n",
    "    - [nn.Sequential](#toc1_3_4_)    \n",
    "    - [nn.Softmax](#toc1_3_5_)    \n",
    "  - [Model parameters](#toc1_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Building the model layers](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Build a neural network](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[Get a hardware device for training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Define the class](#toc0_)\n",
    "\n",
    "- We define our neural network by subclassing ``nn.Module``, and initialize the neural network layers in ``__init__``.\n",
    "- Every nn.Module subclass implements the operations on input data in the forward method.\n",
    "\n",
    "Our neural network is composed of the following:\n",
    "```\n",
    "1. The input layer with 28x28 or 784 features/pixels.\n",
    "2. The first linear module takes the input 784 features and transforms it to a hidden layer with 512 features.\n",
    "3. The ReLU activation function will be applied in the transformation.\n",
    "4. The second linear module takes 512 features as input from the first hidden layer and transforms it to the next hidden layer with 512 features.\n",
    "5. The ReLU activation function will be applied in the transformation.\n",
    "6. The third linear module take 512 features as input from the second hidden layer and transforms those features to the output layer with 10, which is the number of classes.\n",
    "7. The ReLU activation function will be applied in the transformation.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        print(\"========__init__\")\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"========forward\")\n",
    "        print(\"--x: \", x[0][0])\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        print(\"--logits\", logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========__init__\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建一个 NeuralNetwork 的实例，并将其移动到 device 中，并打印其结构\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========forward\n",
      "--x:  tensor([0.6713, 0.9800, 0.0375, 0.7683, 0.3016, 0.2872, 0.9026, 0.4352, 0.5712,\n",
      "        0.9999, 0.1142, 0.1094, 0.4680, 0.6605, 0.6617, 0.6932, 0.9095, 0.1173,\n",
      "        0.0639, 0.9552, 0.1322, 0.2908, 0.0895, 0.8303, 0.5566, 0.6190, 0.5297,\n",
      "        0.3281])\n",
      "--logits tensor([[0.0000, 0.0897, 0.0335, 0.0000, 0.0000, 0.0297, 0.0000, 0.1087, 0.0000,\n",
      "         0.0085]], grad_fn=<ReluBackward0>)\n",
      "--->  tensor([[0.0000, 0.0897, 0.0335, 0.0000, 0.0000, 0.0297, 0.0000, 0.1087, 0.0000,\n",
      "         0.0085]], grad_fn=<ReluBackward0>)\n",
      "---> Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "# 2. 使用此模型实例\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)          # 调用模型会返回一个 10 维张量，其中包含每个类别的原始预测值\n",
    "print(\"---> \", logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)     # 把值传递给 nn.Softmax 的实例来获取预测密度\n",
    "y_pred = pred_probab.argmax(1) \n",
    "print(f\"---> Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0218, 0.0875, 0.0030, 0.0212, 0.0000, 0.1574, 0.0776, 0.0228,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Weight and Bias](#toc0_)\n",
    "\n",
    "- nn.Linear 模块随机初始化每一层的 weights 和 bias 并在内部将值存储在 Tensors 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear weights: Parameter containing:\n",
      "tensor([[-3.3600e-03,  1.7673e-02, -1.0084e-02,  ...,  2.1328e-02,\n",
      "          1.2723e-02, -2.0640e-02],\n",
      "        [ 1.9739e-02, -3.2739e-02, -1.1304e-02,  ...,  1.6896e-02,\n",
      "         -1.0160e-03, -2.7725e-03],\n",
      "        [ 1.6177e-02,  3.2788e-02,  2.0461e-02,  ..., -1.7207e-02,\n",
      "         -1.2042e-02, -2.1971e-02],\n",
      "        ...,\n",
      "        [ 1.3602e-02, -2.0067e-02,  3.3359e-03,  ..., -7.0133e-03,\n",
      "         -2.1222e-02, -3.1736e-04],\n",
      "        [ 1.2792e-02, -4.7015e-03,  1.8607e-02,  ..., -2.7154e-05,\n",
      "          2.4730e-02,  2.0821e-02],\n",
      "        [-5.8050e-03,  2.4799e-02, -1.1905e-04,  ...,  8.8574e-03,\n",
      "          2.1331e-02,  2.0435e-02]], requires_grad=True) \n",
      "\n",
      "First Linear biases: Parameter containing:\n",
      "tensor([-1.3724e-02,  2.4594e-02, -9.8472e-03, -1.4274e-03, -3.3574e-02,\n",
      "         1.8288e-02, -4.6431e-03, -2.9256e-02, -2.4660e-02,  2.5854e-02,\n",
      "         8.3790e-03, -5.5297e-03, -1.4865e-02, -2.8793e-02, -9.7110e-03,\n",
      "        -9.9543e-03, -1.7029e-02, -2.9877e-02, -3.3707e-02, -4.6175e-03,\n",
      "         1.1821e-02, -1.5165e-02, -1.1299e-02, -9.5896e-03, -3.5014e-03,\n",
      "        -2.5584e-02, -3.0604e-02,  1.8736e-02, -1.0695e-04, -1.2587e-02,\n",
      "        -1.8721e-02,  3.0752e-02, -6.8136e-03,  3.2239e-02, -2.8797e-02,\n",
      "        -3.1933e-02,  7.1386e-03, -3.5179e-02,  2.9760e-02, -1.1875e-02,\n",
      "         2.3517e-02,  2.7182e-03, -2.0206e-04, -3.0017e-02, -2.8951e-02,\n",
      "        -1.3643e-02,  3.6804e-04,  3.2575e-02, -9.7330e-03,  2.1374e-03,\n",
      "         1.8869e-02, -2.6199e-02, -1.3366e-02,  7.7994e-03, -3.4521e-02,\n",
      "        -1.8655e-02, -3.5199e-02,  3.3199e-03, -3.3901e-02,  3.8466e-03,\n",
      "        -1.3385e-02, -1.5343e-02, -5.4146e-03,  2.6246e-03,  1.8901e-02,\n",
      "         1.1731e-02,  2.1077e-02, -6.0863e-03,  7.3673e-03,  2.1542e-02,\n",
      "         5.7500e-03, -3.4045e-02, -2.6491e-02, -1.3832e-02,  1.2558e-02,\n",
      "         1.3399e-03, -2.6097e-02,  1.0617e-02,  1.5895e-02, -3.1325e-02,\n",
      "         2.0016e-04,  2.2330e-02, -7.0747e-03, -2.9365e-02,  5.9238e-03,\n",
      "        -8.0756e-03,  2.4557e-02,  9.9755e-03,  1.6792e-02,  1.5065e-02,\n",
      "         2.2843e-02,  2.2848e-02,  1.4151e-02,  1.1112e-02,  2.5848e-02,\n",
      "        -2.3889e-02,  2.3207e-02, -8.9306e-03, -4.7738e-04,  1.9764e-03,\n",
      "         2.2776e-02, -2.4089e-02, -8.3604e-03, -2.3852e-02, -2.6554e-02,\n",
      "        -1.3985e-02, -2.7333e-03,  3.1585e-02,  1.3958e-02, -3.3829e-02,\n",
      "         2.1708e-02,  9.0441e-03, -2.1291e-02,  1.4704e-02, -1.7378e-02,\n",
      "         2.6359e-02, -3.7483e-03,  2.0902e-02, -2.9099e-02,  2.4834e-03,\n",
      "         1.0932e-02, -8.6489e-03, -3.4659e-02, -3.3452e-02, -1.9915e-02,\n",
      "         3.5497e-02,  6.9675e-03,  9.5753e-04, -2.6554e-02,  2.7172e-02,\n",
      "        -1.4643e-02, -2.9705e-02,  2.0964e-02,  3.3732e-02,  1.7806e-02,\n",
      "        -2.6092e-02,  3.1396e-02,  1.9014e-02,  3.4825e-02,  2.7557e-02,\n",
      "        -2.6330e-02, -2.7616e-02,  2.4367e-02, -2.8481e-02,  1.5070e-02,\n",
      "        -4.3874e-03,  5.2775e-03,  3.0770e-02,  1.4227e-02,  7.6173e-03,\n",
      "        -3.4249e-02,  2.3436e-02,  1.1399e-02,  1.5227e-02,  1.4903e-02,\n",
      "         3.3864e-02,  2.3482e-02,  2.3010e-02, -1.3019e-02, -2.9578e-02,\n",
      "         8.4314e-03,  2.4553e-02,  6.7511e-03,  1.8496e-02,  9.1970e-03,\n",
      "         4.0443e-03, -3.3206e-02,  2.0975e-02,  1.0400e-02,  2.0328e-02,\n",
      "         5.3574e-03,  1.6788e-02, -2.6733e-02,  1.5967e-02, -2.3281e-02,\n",
      "         3.1228e-03, -3.1412e-02,  2.7039e-02,  1.3617e-02, -2.3000e-02,\n",
      "        -2.0106e-02, -8.1055e-03, -2.1523e-02, -1.6547e-02,  1.5992e-02,\n",
      "         1.3057e-02, -2.4292e-02,  6.9363e-03, -2.7257e-02, -3.2258e-02,\n",
      "         1.0565e-02,  1.1955e-02,  1.4502e-02, -3.4264e-02,  6.6266e-03,\n",
      "         9.0974e-03, -7.5557e-03,  2.7622e-02, -1.7040e-02, -1.2335e-02,\n",
      "        -2.2263e-02, -2.8329e-03,  1.3063e-03, -5.5477e-04,  4.4080e-03,\n",
      "        -1.9787e-02, -7.8205e-03,  5.7027e-03, -6.1480e-03,  6.5418e-03,\n",
      "        -1.3728e-02,  1.2467e-02, -3.5551e-02,  3.1831e-02, -1.1046e-02,\n",
      "         2.1804e-02,  1.3421e-02, -3.2490e-03, -2.5818e-03,  1.6375e-02,\n",
      "        -2.0869e-03,  3.3040e-02, -2.9853e-02, -2.9072e-02, -3.2484e-02,\n",
      "         2.2810e-04,  1.2345e-02, -2.4526e-02,  1.0595e-02,  8.4413e-03,\n",
      "         3.1772e-02, -2.5163e-02, -1.9231e-03, -2.9009e-02, -3.4854e-02,\n",
      "        -3.3719e-02,  1.5226e-02, -1.8088e-02, -5.8484e-03, -3.1816e-02,\n",
      "        -1.3482e-02, -8.0905e-03,  2.1992e-02, -1.2308e-02,  1.8423e-02,\n",
      "        -3.7444e-03, -1.3346e-02, -2.3407e-03,  1.7971e-02, -3.4493e-03,\n",
      "        -2.7323e-02,  3.4309e-02,  3.0908e-02, -1.3005e-02,  6.7041e-03,\n",
      "         2.1464e-02, -2.7320e-02, -2.7988e-02,  2.4437e-02, -2.2629e-02,\n",
      "         7.5734e-03,  2.7071e-02, -2.0723e-02,  2.2164e-02, -1.0939e-02,\n",
      "        -3.1059e-02,  3.0879e-03, -1.3416e-02, -8.3765e-03,  2.8849e-02,\n",
      "         7.4189e-03,  2.4384e-02, -2.3484e-02,  2.8521e-02, -2.6326e-02,\n",
      "         3.9697e-04, -2.4468e-02, -2.0795e-03,  2.3797e-02, -2.7042e-04,\n",
      "        -6.3667e-03, -3.4949e-02, -3.2680e-02,  1.6073e-02,  2.7220e-02,\n",
      "         3.0160e-02, -3.3118e-02,  3.6244e-03,  1.0165e-03, -9.0783e-03,\n",
      "        -2.8507e-02, -2.7608e-02,  1.9491e-02, -1.1371e-02, -1.4393e-02,\n",
      "        -2.8045e-02, -4.7553e-04,  2.8905e-02,  2.9014e-02, -3.0031e-02,\n",
      "         3.9523e-03,  2.9437e-02,  2.3914e-02, -2.8135e-02, -3.3072e-02,\n",
      "        -1.8054e-02, -7.1600e-03, -7.9326e-03, -1.5827e-02,  1.2481e-03,\n",
      "         2.9672e-02,  3.3777e-02, -1.3003e-02,  3.3006e-02,  1.6270e-02,\n",
      "         2.1040e-02, -3.3659e-02,  7.3388e-03,  7.8683e-03,  3.8599e-03,\n",
      "        -3.4977e-02, -8.1957e-03,  1.8457e-02, -1.3231e-02, -3.1713e-02,\n",
      "         2.4593e-02,  6.7556e-03,  2.9345e-02,  3.0263e-02,  7.0575e-03,\n",
      "         1.3073e-02, -2.2258e-02, -9.1573e-03, -4.4490e-03,  1.9362e-02,\n",
      "        -1.9597e-02,  5.1040e-03,  2.7903e-02,  2.5376e-02,  2.7461e-02,\n",
      "        -2.4787e-02,  1.5634e-03, -3.2758e-02,  2.1334e-02,  5.1159e-03,\n",
      "        -4.2560e-03, -3.4215e-02,  2.7538e-02, -2.1343e-02, -4.3605e-05,\n",
      "        -1.7860e-02, -2.0794e-02, -1.2504e-02,  3.2226e-02, -3.2417e-02,\n",
      "         3.1458e-02,  7.0510e-03, -2.1475e-03, -2.9053e-02, -2.6046e-02,\n",
      "         1.3327e-02,  1.9001e-02,  2.9700e-02,  1.1735e-02,  2.1887e-03,\n",
      "        -1.4396e-02, -2.5684e-03, -2.6730e-02,  9.4681e-03,  1.7602e-02,\n",
      "        -2.5725e-02,  3.3676e-03,  1.2683e-02, -1.1340e-02, -2.2875e-02,\n",
      "         1.0393e-02, -3.8526e-03, -2.4412e-02, -1.2635e-02, -5.3849e-03,\n",
      "         1.1558e-02,  3.2247e-02,  1.9243e-02, -3.1835e-02,  2.1369e-02,\n",
      "        -2.6821e-02,  3.4079e-02, -6.9327e-03,  3.3071e-02, -1.5009e-03,\n",
      "         5.3689e-03, -3.2317e-02,  6.6776e-03, -9.2758e-03,  3.5359e-03,\n",
      "        -1.4989e-02,  1.8853e-02, -9.9723e-03,  3.0602e-02, -2.5319e-02,\n",
      "         3.4768e-02, -3.0416e-02, -2.7519e-03,  1.7798e-02,  9.4213e-03,\n",
      "         1.5348e-02,  2.5038e-02, -3.4324e-02,  1.2883e-02,  2.0752e-02,\n",
      "         2.5048e-02, -2.0366e-02,  1.5534e-02,  2.9466e-02,  3.6656e-03,\n",
      "         2.3613e-02,  1.6306e-02, -2.1044e-02,  2.1228e-02,  4.7387e-03,\n",
      "        -4.1242e-03, -1.7002e-03,  1.4109e-02,  2.3581e-02, -7.6271e-03,\n",
      "        -3.3380e-02, -2.6213e-04, -3.0227e-02, -2.6878e-02, -1.9923e-02,\n",
      "        -1.2403e-02,  3.2396e-02, -1.8520e-02, -5.8283e-03, -3.4716e-02,\n",
      "        -4.4113e-03,  2.8370e-02, -9.2179e-03,  7.2008e-03,  3.3301e-02,\n",
      "         1.2879e-02, -7.0320e-03, -1.8488e-02,  4.1494e-03,  3.2108e-02,\n",
      "        -2.5989e-02, -7.4439e-03,  7.8847e-03,  2.3102e-02,  5.0654e-03,\n",
      "        -2.3301e-02,  1.0812e-02,  2.3746e-02,  3.4648e-03,  4.4450e-03,\n",
      "         2.8240e-02, -1.1910e-02,  2.2688e-02, -1.2206e-02,  1.3811e-03,\n",
      "         2.4970e-02,  3.3541e-02, -4.3032e-03, -1.5672e-02, -3.0420e-02,\n",
      "         2.5244e-02, -1.8051e-02, -1.5314e-02,  2.3036e-02, -2.0418e-03,\n",
      "        -1.3512e-02, -2.7705e-02,  1.3808e-02,  1.2305e-02, -2.8702e-02,\n",
      "         1.6676e-02,  1.6561e-02, -2.3516e-02,  1.3230e-02, -5.9929e-03,\n",
      "        -3.0549e-02,  2.5608e-02, -1.8918e-02,  9.0140e-03,  2.5524e-02,\n",
      "         1.2991e-02, -8.2711e-03, -3.6261e-04, -2.5624e-02, -2.0931e-02,\n",
      "        -1.5101e-02,  1.9368e-02,  1.5936e-02,  2.3688e-02,  5.4554e-03,\n",
      "         1.7143e-02,  1.8063e-02,  5.0723e-03, -2.5645e-02,  5.7086e-03,\n",
      "        -2.2630e-02,  1.6602e-02, -4.9868e-03, -1.7269e-02, -1.3800e-02,\n",
      "        -2.5982e-02,  3.3280e-02, -2.6420e-02,  2.6335e-02,  6.0110e-03,\n",
      "         3.5315e-02,  3.4849e-02], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
    "\n",
    "print(f\"First Linear biases: {model.linear_relu_stack[0].bias} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Model layers](#toc0_)\n",
    "\n",
    "- 分解 FashionMNIST 模型中的各个层。\n",
    "- 为了说明这一点，我们将采用 3 张大小为 28x28 的图像的小批量样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[nn.Flatten](#toc0_)\n",
    "\n",
    "- 初始化 nn.Flatten 层，将每个 2D 28x28 图像转换为 784 个像素值的连续数组，即维持小批量维度（dim=0 时）\n",
    "- 每个像素都被传递到神经网络的输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[nn.Linear](#toc0_)\n",
    "\n",
    "- 线性层是一个使用其存储的权重和偏差对输入应用线性变换的模块\n",
    "- 输入层中每个像素的灰度值将连接到隐藏层中的神经元进行计算\n",
    "- 用于转换的计算是 weight∗input+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a>[nn.ReLU](#toc0_)\n",
    "\n",
    "- 非线性激活是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后应用以引入非线性，帮助神经网络学习各种现象。\n",
    "- 在此模型中，我们在线性层之间使用 nn.ReLU\n",
    "- ReLU 激活函数获取线性层计算的输出，并将负值替换为零。\n",
    "\n",
    "\n",
    "Linear output: ${ x = {weight * input + bias}} $.  \n",
    "ReLU:  $f(x)= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    x, & \\text{if } x\\geq 0\\\\\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.4944, -0.1607, -0.1401,  0.4301,  0.5532,  0.0578,  0.3011, -0.0024,\n",
      "         -0.4917, -0.1941, -0.2191, -0.5344, -0.4400, -0.2563, -0.1389,  0.4899,\n",
      "          0.2086,  0.0346,  0.2361,  0.2533],\n",
      "        [ 0.1026, -0.4872, -0.2421,  0.4621,  0.0791,  0.5096,  0.3423,  0.1377,\n",
      "         -0.6847, -0.0187, -0.2458, -0.1424, -0.1524, -0.0881, -0.1300,  0.2688,\n",
      "          0.1321, -0.0317,  0.0481,  0.1814],\n",
      "        [ 0.3747, -0.3533,  0.0103,  0.6814,  0.3014,  0.3590,  0.4004,  0.1450,\n",
      "         -0.9122, -0.2040, -0.5909, -0.0821, -0.2452,  0.1204, -0.0147,  0.3103,\n",
      "          0.2773, -0.0685, -0.0464, -0.1741]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.4944, 0.0000, 0.0000, 0.4301, 0.5532, 0.0578, 0.3011, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4899, 0.2086, 0.0346,\n",
      "         0.2361, 0.2533],\n",
      "        [0.1026, 0.0000, 0.0000, 0.4621, 0.0791, 0.5096, 0.3423, 0.1377, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2688, 0.1321, 0.0000,\n",
      "         0.0481, 0.1814],\n",
      "        [0.3747, 0.0000, 0.0103, 0.6814, 0.3014, 0.3590, 0.4004, 0.1450, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1204, 0.0000, 0.3103, 0.2773, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[nn.Sequential](#toc0_)\n",
    "\n",
    "- nn.Sequential 是模块的有序容器。数据按照定义的顺序通过所有模块\n",
    "- 您可以使用顺序容器来组合一个快速网络，例如 seq_modules\n",
    "\n",
    "注：这个是可以把其他的组合在一起\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1620,  0.3904, -0.3265,  0.0129,  0.0988,  0.0296, -0.2605,  0.0922,\n",
       "          0.2681, -0.2154],\n",
       "        [ 0.1186,  0.4392, -0.3286,  0.2184,  0.0560, -0.2330, -0.3739,  0.0420,\n",
       "          0.1948, -0.2438],\n",
       "        [ 0.0392,  0.2726, -0.3239,  0.1041, -0.0461, -0.0214, -0.1551, -0.1006,\n",
       "          0.0993,  0.0305]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=28*28, out_features=20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_5_'></a>[nn.Softmax](#toc0_)\n",
    "\n",
    "- Softmax 激活函数用于计算神经网络输出的概率。\n",
    "- 它仅用于神经网络的输出层。\n",
    "- 结果将缩放到值 \\[0， 1\\]，表示模型对每个类的预测密度。\n",
    "- “dim”参数指示结果值必须总和 1 的维度。概率最高的节点预测所需的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1120, 0.1407, 0.0687, 0.0965, 0.1051, 0.0981, 0.0734, 0.1044, 0.1245,\n",
       "         0.0768],\n",
       "        [0.1102, 0.1518, 0.0704, 0.1217, 0.1035, 0.0775, 0.0673, 0.1020, 0.1189,\n",
       "         0.0767],\n",
       "        [0.1038, 0.1311, 0.0722, 0.1108, 0.0953, 0.0977, 0.0855, 0.0903, 0.1103,\n",
       "         0.1029]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Model parameters](#toc0_)\n",
    "\n",
    "- 神经网络内的许多层都是参数化的，也就是说，这些层具有在训练期间优化的关联权重和偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0034,  0.0177, -0.0101,  ...,  0.0213,  0.0127, -0.0206],\n",
      "        [ 0.0197, -0.0327, -0.0113,  ...,  0.0169, -0.0010, -0.0028]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0137,  0.0246], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0213, -0.0405, -0.0068,  ...,  0.0376,  0.0434, -0.0058],\n",
      "        [ 0.0183, -0.0006,  0.0382,  ..., -0.0136,  0.0409, -0.0433]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0028, -0.0099], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0398,  0.0096,  0.0439,  ...,  0.0027, -0.0038, -0.0065],\n",
      "        [-0.0264,  0.0280,  0.0433,  ...,  0.0222,  0.0104,  0.0427]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0093,  0.0173], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "# 迭代每个参数，并打印其大小及其值的预览\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
      "tensor([[-3.3600e-03,  1.7673e-02, -1.0084e-02,  ...,  2.1328e-02,\n",
      "          1.2723e-02, -2.0640e-02],\n",
      "        [ 1.9739e-02, -3.2739e-02, -1.1304e-02,  ...,  1.6896e-02,\n",
      "         -1.0160e-03, -2.7725e-03],\n",
      "        [ 1.6177e-02,  3.2788e-02,  2.0461e-02,  ..., -1.7207e-02,\n",
      "         -1.2042e-02, -2.1971e-02],\n",
      "        ...,\n",
      "        [ 1.3602e-02, -2.0067e-02,  3.3359e-03,  ..., -7.0133e-03,\n",
      "         -2.1222e-02, -3.1736e-04],\n",
      "        [ 1.2792e-02, -4.7015e-03,  1.8607e-02,  ..., -2.7154e-05,\n",
      "          2.4730e-02,  2.0821e-02],\n",
      "        [-5.8050e-03,  2.4799e-02, -1.1905e-04,  ...,  8.8574e-03,\n",
      "          2.1331e-02,  2.0435e-02]], requires_grad=True) \n",
      "\n",
      "=================\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([-1.3724e-02,  2.4594e-02, -9.8472e-03, -1.4274e-03, -3.3574e-02,\n",
      "         1.8288e-02, -4.6431e-03, -2.9256e-02, -2.4660e-02,  2.5854e-02,\n",
      "         8.3790e-03, -5.5297e-03, -1.4865e-02, -2.8793e-02, -9.7110e-03,\n",
      "        -9.9543e-03, -1.7029e-02, -2.9877e-02, -3.3707e-02, -4.6175e-03,\n",
      "         1.1821e-02, -1.5165e-02, -1.1299e-02, -9.5896e-03, -3.5014e-03,\n",
      "        -2.5584e-02, -3.0604e-02,  1.8736e-02, -1.0695e-04, -1.2587e-02,\n",
      "        -1.8721e-02,  3.0752e-02, -6.8136e-03,  3.2239e-02, -2.8797e-02,\n",
      "        -3.1933e-02,  7.1386e-03, -3.5179e-02,  2.9760e-02, -1.1875e-02,\n",
      "         2.3517e-02,  2.7182e-03, -2.0206e-04, -3.0017e-02, -2.8951e-02,\n",
      "        -1.3643e-02,  3.6804e-04,  3.2575e-02, -9.7330e-03,  2.1374e-03,\n",
      "         1.8869e-02, -2.6199e-02, -1.3366e-02,  7.7994e-03, -3.4521e-02,\n",
      "        -1.8655e-02, -3.5199e-02,  3.3199e-03, -3.3901e-02,  3.8466e-03,\n",
      "        -1.3385e-02, -1.5343e-02, -5.4146e-03,  2.6246e-03,  1.8901e-02,\n",
      "         1.1731e-02,  2.1077e-02, -6.0863e-03,  7.3673e-03,  2.1542e-02,\n",
      "         5.7500e-03, -3.4045e-02, -2.6491e-02, -1.3832e-02,  1.2558e-02,\n",
      "         1.3399e-03, -2.6097e-02,  1.0617e-02,  1.5895e-02, -3.1325e-02,\n",
      "         2.0016e-04,  2.2330e-02, -7.0747e-03, -2.9365e-02,  5.9238e-03,\n",
      "        -8.0756e-03,  2.4557e-02,  9.9755e-03,  1.6792e-02,  1.5065e-02,\n",
      "         2.2843e-02,  2.2848e-02,  1.4151e-02,  1.1112e-02,  2.5848e-02,\n",
      "        -2.3889e-02,  2.3207e-02, -8.9306e-03, -4.7738e-04,  1.9764e-03,\n",
      "         2.2776e-02, -2.4089e-02, -8.3604e-03, -2.3852e-02, -2.6554e-02,\n",
      "        -1.3985e-02, -2.7333e-03,  3.1585e-02,  1.3958e-02, -3.3829e-02,\n",
      "         2.1708e-02,  9.0441e-03, -2.1291e-02,  1.4704e-02, -1.7378e-02,\n",
      "         2.6359e-02, -3.7483e-03,  2.0902e-02, -2.9099e-02,  2.4834e-03,\n",
      "         1.0932e-02, -8.6489e-03, -3.4659e-02, -3.3452e-02, -1.9915e-02,\n",
      "         3.5497e-02,  6.9675e-03,  9.5753e-04, -2.6554e-02,  2.7172e-02,\n",
      "        -1.4643e-02, -2.9705e-02,  2.0964e-02,  3.3732e-02,  1.7806e-02,\n",
      "        -2.6092e-02,  3.1396e-02,  1.9014e-02,  3.4825e-02,  2.7557e-02,\n",
      "        -2.6330e-02, -2.7616e-02,  2.4367e-02, -2.8481e-02,  1.5070e-02,\n",
      "        -4.3874e-03,  5.2775e-03,  3.0770e-02,  1.4227e-02,  7.6173e-03,\n",
      "        -3.4249e-02,  2.3436e-02,  1.1399e-02,  1.5227e-02,  1.4903e-02,\n",
      "         3.3864e-02,  2.3482e-02,  2.3010e-02, -1.3019e-02, -2.9578e-02,\n",
      "         8.4314e-03,  2.4553e-02,  6.7511e-03,  1.8496e-02,  9.1970e-03,\n",
      "         4.0443e-03, -3.3206e-02,  2.0975e-02,  1.0400e-02,  2.0328e-02,\n",
      "         5.3574e-03,  1.6788e-02, -2.6733e-02,  1.5967e-02, -2.3281e-02,\n",
      "         3.1228e-03, -3.1412e-02,  2.7039e-02,  1.3617e-02, -2.3000e-02,\n",
      "        -2.0106e-02, -8.1055e-03, -2.1523e-02, -1.6547e-02,  1.5992e-02,\n",
      "         1.3057e-02, -2.4292e-02,  6.9363e-03, -2.7257e-02, -3.2258e-02,\n",
      "         1.0565e-02,  1.1955e-02,  1.4502e-02, -3.4264e-02,  6.6266e-03,\n",
      "         9.0974e-03, -7.5557e-03,  2.7622e-02, -1.7040e-02, -1.2335e-02,\n",
      "        -2.2263e-02, -2.8329e-03,  1.3063e-03, -5.5477e-04,  4.4080e-03,\n",
      "        -1.9787e-02, -7.8205e-03,  5.7027e-03, -6.1480e-03,  6.5418e-03,\n",
      "        -1.3728e-02,  1.2467e-02, -3.5551e-02,  3.1831e-02, -1.1046e-02,\n",
      "         2.1804e-02,  1.3421e-02, -3.2490e-03, -2.5818e-03,  1.6375e-02,\n",
      "        -2.0869e-03,  3.3040e-02, -2.9853e-02, -2.9072e-02, -3.2484e-02,\n",
      "         2.2810e-04,  1.2345e-02, -2.4526e-02,  1.0595e-02,  8.4413e-03,\n",
      "         3.1772e-02, -2.5163e-02, -1.9231e-03, -2.9009e-02, -3.4854e-02,\n",
      "        -3.3719e-02,  1.5226e-02, -1.8088e-02, -5.8484e-03, -3.1816e-02,\n",
      "        -1.3482e-02, -8.0905e-03,  2.1992e-02, -1.2308e-02,  1.8423e-02,\n",
      "        -3.7444e-03, -1.3346e-02, -2.3407e-03,  1.7971e-02, -3.4493e-03,\n",
      "        -2.7323e-02,  3.4309e-02,  3.0908e-02, -1.3005e-02,  6.7041e-03,\n",
      "         2.1464e-02, -2.7320e-02, -2.7988e-02,  2.4437e-02, -2.2629e-02,\n",
      "         7.5734e-03,  2.7071e-02, -2.0723e-02,  2.2164e-02, -1.0939e-02,\n",
      "        -3.1059e-02,  3.0879e-03, -1.3416e-02, -8.3765e-03,  2.8849e-02,\n",
      "         7.4189e-03,  2.4384e-02, -2.3484e-02,  2.8521e-02, -2.6326e-02,\n",
      "         3.9697e-04, -2.4468e-02, -2.0795e-03,  2.3797e-02, -2.7042e-04,\n",
      "        -6.3667e-03, -3.4949e-02, -3.2680e-02,  1.6073e-02,  2.7220e-02,\n",
      "         3.0160e-02, -3.3118e-02,  3.6244e-03,  1.0165e-03, -9.0783e-03,\n",
      "        -2.8507e-02, -2.7608e-02,  1.9491e-02, -1.1371e-02, -1.4393e-02,\n",
      "        -2.8045e-02, -4.7553e-04,  2.8905e-02,  2.9014e-02, -3.0031e-02,\n",
      "         3.9523e-03,  2.9437e-02,  2.3914e-02, -2.8135e-02, -3.3072e-02,\n",
      "        -1.8054e-02, -7.1600e-03, -7.9326e-03, -1.5827e-02,  1.2481e-03,\n",
      "         2.9672e-02,  3.3777e-02, -1.3003e-02,  3.3006e-02,  1.6270e-02,\n",
      "         2.1040e-02, -3.3659e-02,  7.3388e-03,  7.8683e-03,  3.8599e-03,\n",
      "        -3.4977e-02, -8.1957e-03,  1.8457e-02, -1.3231e-02, -3.1713e-02,\n",
      "         2.4593e-02,  6.7556e-03,  2.9345e-02,  3.0263e-02,  7.0575e-03,\n",
      "         1.3073e-02, -2.2258e-02, -9.1573e-03, -4.4490e-03,  1.9362e-02,\n",
      "        -1.9597e-02,  5.1040e-03,  2.7903e-02,  2.5376e-02,  2.7461e-02,\n",
      "        -2.4787e-02,  1.5634e-03, -3.2758e-02,  2.1334e-02,  5.1159e-03,\n",
      "        -4.2560e-03, -3.4215e-02,  2.7538e-02, -2.1343e-02, -4.3605e-05,\n",
      "        -1.7860e-02, -2.0794e-02, -1.2504e-02,  3.2226e-02, -3.2417e-02,\n",
      "         3.1458e-02,  7.0510e-03, -2.1475e-03, -2.9053e-02, -2.6046e-02,\n",
      "         1.3327e-02,  1.9001e-02,  2.9700e-02,  1.1735e-02,  2.1887e-03,\n",
      "        -1.4396e-02, -2.5684e-03, -2.6730e-02,  9.4681e-03,  1.7602e-02,\n",
      "        -2.5725e-02,  3.3676e-03,  1.2683e-02, -1.1340e-02, -2.2875e-02,\n",
      "         1.0393e-02, -3.8526e-03, -2.4412e-02, -1.2635e-02, -5.3849e-03,\n",
      "         1.1558e-02,  3.2247e-02,  1.9243e-02, -3.1835e-02,  2.1369e-02,\n",
      "        -2.6821e-02,  3.4079e-02, -6.9327e-03,  3.3071e-02, -1.5009e-03,\n",
      "         5.3689e-03, -3.2317e-02,  6.6776e-03, -9.2758e-03,  3.5359e-03,\n",
      "        -1.4989e-02,  1.8853e-02, -9.9723e-03,  3.0602e-02, -2.5319e-02,\n",
      "         3.4768e-02, -3.0416e-02, -2.7519e-03,  1.7798e-02,  9.4213e-03,\n",
      "         1.5348e-02,  2.5038e-02, -3.4324e-02,  1.2883e-02,  2.0752e-02,\n",
      "         2.5048e-02, -2.0366e-02,  1.5534e-02,  2.9466e-02,  3.6656e-03,\n",
      "         2.3613e-02,  1.6306e-02, -2.1044e-02,  2.1228e-02,  4.7387e-03,\n",
      "        -4.1242e-03, -1.7002e-03,  1.4109e-02,  2.3581e-02, -7.6271e-03,\n",
      "        -3.3380e-02, -2.6213e-04, -3.0227e-02, -2.6878e-02, -1.9923e-02,\n",
      "        -1.2403e-02,  3.2396e-02, -1.8520e-02, -5.8283e-03, -3.4716e-02,\n",
      "        -4.4113e-03,  2.8370e-02, -9.2179e-03,  7.2008e-03,  3.3301e-02,\n",
      "         1.2879e-02, -7.0320e-03, -1.8488e-02,  4.1494e-03,  3.2108e-02,\n",
      "        -2.5989e-02, -7.4439e-03,  7.8847e-03,  2.3102e-02,  5.0654e-03,\n",
      "        -2.3301e-02,  1.0812e-02,  2.3746e-02,  3.4648e-03,  4.4450e-03,\n",
      "         2.8240e-02, -1.1910e-02,  2.2688e-02, -1.2206e-02,  1.3811e-03,\n",
      "         2.4970e-02,  3.3541e-02, -4.3032e-03, -1.5672e-02, -3.0420e-02,\n",
      "         2.5244e-02, -1.8051e-02, -1.5314e-02,  2.3036e-02, -2.0418e-03,\n",
      "        -1.3512e-02, -2.7705e-02,  1.3808e-02,  1.2305e-02, -2.8702e-02,\n",
      "         1.6676e-02,  1.6561e-02, -2.3516e-02,  1.3230e-02, -5.9929e-03,\n",
      "        -3.0549e-02,  2.5608e-02, -1.8918e-02,  9.0140e-03,  2.5524e-02,\n",
      "         1.2991e-02, -8.2711e-03, -3.6261e-04, -2.5624e-02, -2.0931e-02,\n",
      "        -1.5101e-02,  1.9368e-02,  1.5936e-02,  2.3688e-02,  5.4554e-03,\n",
      "         1.7143e-02,  1.8063e-02,  5.0723e-03, -2.5645e-02,  5.7086e-03,\n",
      "        -2.2630e-02,  1.6602e-02, -4.9868e-03, -1.7269e-02, -1.3800e-02,\n",
      "        -2.5982e-02,  3.3280e-02, -2.6420e-02,  2.6335e-02,  6.0110e-03,\n",
      "         3.5315e-02,  3.4849e-02], requires_grad=True) \n",
      "\n",
      "=================\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0213, -0.0405, -0.0068,  ...,  0.0376,  0.0434, -0.0058],\n",
      "        [ 0.0183, -0.0006,  0.0382,  ..., -0.0136,  0.0409, -0.0433],\n",
      "        [-0.0237, -0.0082,  0.0385,  ..., -0.0380, -0.0029, -0.0145],\n",
      "        ...,\n",
      "        [ 0.0066, -0.0145,  0.0362,  ..., -0.0181, -0.0259,  0.0117],\n",
      "        [ 0.0249,  0.0385, -0.0358,  ...,  0.0044, -0.0100, -0.0347],\n",
      "        [ 0.0412, -0.0116,  0.0274,  ..., -0.0245, -0.0279,  0.0428]],\n",
      "       requires_grad=True) \n",
      "\n",
      "=================\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([-2.8483e-03, -9.8990e-03, -3.7001e-02, -3.6767e-02,  7.5691e-03,\n",
      "         6.7644e-03, -3.5633e-03,  4.3421e-02,  3.6724e-02, -4.3547e-02,\n",
      "        -4.1530e-02,  7.2845e-03,  1.2337e-02, -1.9159e-02, -1.4944e-02,\n",
      "         2.2403e-02, -2.3033e-02, -2.5934e-02, -1.8062e-02,  3.0647e-02,\n",
      "         1.3639e-02,  1.8900e-02, -2.8855e-02,  3.8277e-02,  3.5149e-02,\n",
      "         2.2449e-02, -8.3076e-03, -1.1709e-03, -3.2759e-02, -1.5143e-02,\n",
      "         2.2987e-02,  3.4581e-02, -5.0570e-04, -2.7730e-02,  1.2043e-04,\n",
      "        -2.9893e-03, -3.2571e-02, -1.6614e-02,  4.3800e-02, -2.8019e-02,\n",
      "        -2.7941e-03,  2.5611e-02,  2.3741e-02,  3.2466e-02, -1.6588e-02,\n",
      "         4.1498e-02, -2.8968e-02, -4.0158e-02,  2.9428e-02, -3.6263e-02,\n",
      "         2.7757e-02, -2.7868e-02,  2.4364e-02, -2.5903e-02, -2.9379e-02,\n",
      "        -2.2096e-02, -3.0822e-03, -8.4797e-03, -2.3518e-02,  6.1974e-03,\n",
      "         3.6619e-02, -1.8696e-02,  1.5705e-02, -3.6145e-02, -3.4559e-02,\n",
      "        -3.1113e-02, -1.4878e-02, -3.2228e-02, -2.2480e-03, -2.7093e-02,\n",
      "         2.5782e-03,  1.2669e-02,  8.5828e-03,  3.0377e-02, -1.2997e-03,\n",
      "        -3.4175e-02,  1.1393e-02, -3.5765e-02, -2.7916e-02, -3.7709e-03,\n",
      "         1.3153e-02,  5.1956e-04, -3.5814e-02, -3.2066e-02, -3.7112e-02,\n",
      "         1.3252e-02, -3.5423e-02,  2.6384e-02,  3.6795e-02, -2.3840e-02,\n",
      "         3.8407e-02,  3.5284e-03,  2.3175e-02,  7.0797e-04, -1.2098e-02,\n",
      "         5.6823e-03,  2.9337e-02, -1.5476e-02, -3.4203e-02, -1.8205e-02,\n",
      "        -2.6643e-02, -3.4873e-02,  2.7812e-02,  1.9950e-02, -5.8469e-03,\n",
      "        -4.0694e-02, -3.2163e-02, -3.1333e-02, -6.4888e-03,  3.7190e-02,\n",
      "         1.0898e-02, -2.7745e-02,  1.7874e-02,  2.7628e-02,  1.6505e-02,\n",
      "         3.4032e-02, -2.8624e-02,  1.4577e-02,  3.0092e-02, -2.7793e-02,\n",
      "        -3.5344e-02,  8.3793e-03,  2.9181e-02, -2.5259e-02,  9.1940e-03,\n",
      "        -4.0047e-02,  3.0982e-02,  1.5565e-02,  2.0038e-02, -3.4925e-02,\n",
      "         3.3661e-02,  2.1339e-02, -5.8368e-03,  7.3473e-03,  5.4198e-03,\n",
      "         9.4410e-03,  2.9004e-02, -1.6524e-03,  2.9208e-02, -3.7328e-02,\n",
      "        -6.2491e-03,  1.9041e-02, -4.0872e-02, -1.0741e-02, -1.1206e-02,\n",
      "         1.3562e-02,  2.3203e-02,  3.1782e-02, -1.0183e-02, -1.8213e-02,\n",
      "        -1.6185e-02,  6.8935e-03, -1.2201e-03, -3.2265e-03, -2.8388e-02,\n",
      "         3.0566e-02,  2.7084e-02,  1.7074e-02,  1.2004e-02,  6.3257e-03,\n",
      "         3.4346e-02, -6.7730e-03, -1.9775e-02, -3.2317e-02,  2.3594e-02,\n",
      "         1.0037e-02, -1.5577e-02, -3.8072e-03,  1.8267e-02,  1.8064e-02,\n",
      "        -2.1968e-02,  2.4985e-02,  1.3199e-05, -4.0575e-02, -1.9479e-02,\n",
      "        -4.3413e-02, -1.8670e-02,  7.6365e-03, -1.9898e-02, -2.0692e-02,\n",
      "         3.9693e-02, -1.0283e-02,  2.3149e-02, -3.9983e-02, -4.1414e-02,\n",
      "         3.2272e-02, -1.6477e-02,  2.8912e-02,  2.5832e-02,  1.2485e-02,\n",
      "        -3.9623e-02, -9.0718e-03, -1.3480e-04,  2.6935e-04, -3.7938e-02,\n",
      "        -1.6369e-03, -1.0929e-02,  3.8603e-02,  2.6499e-02,  3.1122e-02,\n",
      "        -2.4248e-02,  2.3745e-02,  4.3148e-02, -1.5888e-02,  8.3456e-03,\n",
      "        -1.1260e-02,  2.1591e-02,  2.6178e-02, -3.5245e-02, -1.4427e-02,\n",
      "        -2.8997e-02, -3.1705e-02,  4.0699e-02,  1.1980e-02, -3.6969e-02,\n",
      "        -1.7206e-03, -3.3334e-02,  2.8668e-02,  1.8303e-02,  2.5507e-02,\n",
      "        -3.8879e-02,  3.5414e-02,  3.8791e-02,  2.2789e-02,  3.8888e-02,\n",
      "         2.9051e-02, -1.4226e-02,  2.2476e-02,  2.3337e-02, -3.3389e-02,\n",
      "         5.7513e-03,  4.2496e-02,  4.3432e-02, -3.4118e-02, -2.4593e-02,\n",
      "        -2.8620e-02, -1.9367e-02, -9.2358e-03, -1.0402e-02,  5.1011e-03,\n",
      "         2.5300e-02,  4.2876e-02,  2.2305e-02, -4.1391e-02, -2.9992e-02,\n",
      "        -3.4545e-02, -3.0309e-02, -2.1365e-02, -7.2642e-03,  1.1302e-02,\n",
      "        -3.1042e-02,  5.9212e-04,  6.3055e-03, -4.2316e-02,  3.3182e-02,\n",
      "        -3.2978e-02, -3.6297e-02,  8.4098e-03, -2.2367e-02,  3.9869e-02,\n",
      "         2.6760e-02, -7.5362e-03,  4.1276e-03, -1.8842e-02, -3.1599e-02,\n",
      "         3.7369e-02,  1.3466e-02, -2.7581e-02,  1.0790e-02, -5.8343e-03,\n",
      "         4.1741e-02, -1.0642e-02,  8.5103e-03, -7.5413e-03, -3.2099e-02,\n",
      "         2.8209e-03, -2.7406e-02, -1.3659e-02, -1.6671e-02, -2.1703e-02,\n",
      "        -1.5941e-02,  4.1051e-02,  9.2987e-03, -3.5045e-02, -2.3476e-02,\n",
      "        -1.5223e-02,  1.2272e-02, -3.8703e-02, -1.4172e-02, -1.9369e-02,\n",
      "         1.6245e-02, -2.6319e-02, -2.4134e-02,  2.7215e-02,  1.5417e-02,\n",
      "        -3.3579e-02, -3.6993e-03,  1.9975e-02,  3.5101e-02,  1.3987e-02,\n",
      "        -1.0135e-02, -4.1538e-02,  3.6209e-02, -4.1204e-02,  2.5418e-02,\n",
      "         1.1865e-02,  1.8310e-02, -3.9249e-02, -3.3762e-02,  1.4673e-02,\n",
      "        -2.2427e-02,  1.3348e-02, -9.0225e-03, -4.3344e-02,  1.2612e-02,\n",
      "         4.1868e-02,  3.9031e-02, -2.2795e-02, -3.1500e-02, -2.8097e-02,\n",
      "        -2.4526e-02,  3.1531e-02, -1.6344e-02,  1.1616e-02,  6.9220e-03,\n",
      "         3.6917e-02, -6.0283e-03, -3.9751e-02,  1.7305e-03, -3.9393e-02,\n",
      "         5.3431e-03, -1.3707e-02, -7.9199e-03, -4.4768e-03, -6.1206e-03,\n",
      "         2.1964e-03, -2.3482e-02,  3.8017e-02,  2.1554e-02, -9.9613e-03,\n",
      "         3.4211e-02, -3.5822e-02, -6.5646e-03, -1.3495e-02, -1.4655e-02,\n",
      "        -3.5126e-02, -4.2415e-02,  2.1175e-02, -9.6909e-03,  6.7463e-03,\n",
      "         1.8707e-02, -3.7101e-02,  8.3726e-03, -2.6575e-02, -1.3246e-02,\n",
      "         9.5231e-03,  2.0524e-02, -2.4488e-03,  4.3781e-02, -3.6299e-02,\n",
      "         3.1899e-02,  6.5965e-03,  1.7901e-02, -1.8532e-02, -3.1517e-02,\n",
      "        -4.0957e-02, -4.2073e-02, -2.5724e-02,  4.0058e-02,  5.6607e-03,\n",
      "        -3.4458e-02,  2.4276e-02, -1.7445e-02,  2.8783e-02, -4.0292e-02,\n",
      "        -3.7432e-02,  4.2777e-02,  8.3828e-03, -1.2346e-02,  2.1789e-02,\n",
      "        -1.6547e-02, -1.7016e-02,  8.0504e-03,  3.5106e-02,  1.0757e-02,\n",
      "        -2.1751e-02, -3.3467e-03, -3.0544e-02,  1.3033e-02, -4.2165e-02,\n",
      "         3.7070e-02, -2.3227e-02, -6.6372e-04, -1.6992e-02,  6.2761e-03,\n",
      "         1.1811e-03, -4.7153e-03,  8.6314e-03, -7.6832e-04, -4.1142e-02,\n",
      "        -3.7046e-03, -3.7880e-02, -3.1674e-02, -1.2519e-02, -4.0214e-02,\n",
      "        -2.0245e-03, -2.6685e-03,  2.8774e-02,  4.3016e-02,  2.0201e-02,\n",
      "        -2.3330e-02,  2.1716e-02, -4.2945e-02, -1.5296e-03, -3.2849e-02,\n",
      "        -7.6802e-03, -3.8091e-02,  3.9294e-02, -2.4700e-02,  3.1096e-02,\n",
      "        -2.9337e-03,  4.2813e-02, -3.2744e-02,  2.3353e-02,  5.0749e-03,\n",
      "         3.9957e-02,  4.3298e-02,  2.6769e-02, -2.6889e-02, -2.7367e-02,\n",
      "         2.1191e-02,  1.3192e-02,  2.7708e-02, -3.6669e-02, -1.0824e-02,\n",
      "         6.5061e-03, -2.9093e-02, -2.1316e-02,  3.7188e-02,  1.3780e-02,\n",
      "        -3.5559e-02,  2.5035e-02,  1.7959e-02,  2.0061e-02, -3.0082e-02,\n",
      "        -3.1963e-02, -3.9887e-03, -3.7150e-02, -2.6774e-02, -3.3535e-02,\n",
      "         3.5346e-02, -1.1321e-02, -4.1491e-02,  3.7665e-02, -1.5919e-02,\n",
      "        -4.7024e-03, -1.2335e-02,  1.6008e-02, -2.7798e-02,  3.4164e-02,\n",
      "        -3.8944e-02,  4.0554e-02,  2.3052e-02, -7.0246e-03,  2.8131e-03,\n",
      "        -1.8099e-02,  1.1089e-03,  7.3181e-04, -1.8345e-02, -1.1487e-02,\n",
      "         2.5281e-02,  2.0065e-02,  2.1389e-02,  2.1480e-02, -4.0072e-02,\n",
      "        -5.2867e-03, -1.4741e-02,  1.0496e-02, -1.0632e-02, -3.9115e-02,\n",
      "        -3.6064e-02, -2.1945e-02,  3.4353e-02, -1.0769e-02, -4.1180e-02,\n",
      "        -5.7605e-03, -2.0476e-02, -2.3602e-02,  3.1885e-02, -2.4129e-02,\n",
      "        -1.3174e-02, -4.0521e-02,  4.0088e-02, -1.6546e-02,  1.9166e-02,\n",
      "        -3.6773e-02, -2.6088e-02,  1.5505e-02,  8.5428e-04,  3.1823e-03,\n",
      "         1.4606e-02, -2.8831e-02, -4.0753e-02,  2.5748e-03,  6.2238e-03,\n",
      "        -2.9217e-03, -4.1200e-02, -4.3857e-02, -4.0418e-02, -1.6826e-02,\n",
      "        -3.1197e-02,  2.9560e-02], requires_grad=True) \n",
      "\n",
      "=================\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
      "tensor([[ 0.0398,  0.0096,  0.0439,  ...,  0.0027, -0.0038, -0.0065],\n",
      "        [-0.0264,  0.0280,  0.0433,  ...,  0.0222,  0.0104,  0.0427],\n",
      "        [ 0.0237,  0.0294, -0.0195,  ..., -0.0027, -0.0382, -0.0040],\n",
      "        ...,\n",
      "        [-0.0225, -0.0207, -0.0056,  ..., -0.0051,  0.0265,  0.0048],\n",
      "        [ 0.0341, -0.0167, -0.0108,  ...,  0.0005,  0.0014,  0.0171],\n",
      "        [ 0.0208,  0.0320, -0.0111,  ...,  0.0360,  0.0109, -0.0393]],\n",
      "       requires_grad=True) \n",
      "\n",
      "=================\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
      "tensor([-0.0093,  0.0173, -0.0296,  0.0226, -0.0013,  0.0430,  0.0297,  0.0345,\n",
      "        -0.0332,  0.0413], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(\"=================\")\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
