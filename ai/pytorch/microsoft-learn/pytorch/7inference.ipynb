{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Load and run model predictions](#toc1_)    \n",
    "  - [Load the model](#toc1_1_)    \n",
    "  - [Model Inference](#toc1_2_)    \n",
    "  - [Exporting the model to ONNX](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: onnxruntime in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (1.16.3)\n",
      "Requirement already satisfied: flatbuffers in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (1.23.3)\n",
      "Requirement already satisfied: protobuf in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (4.25.3)\n",
      "Requirement already satisfied: coloredlogs in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: sympy in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: packaging in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/zhaoweiguo/9tool/miniconda3/envs/openCV/lib/python3.8/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import onnxruntime\n",
    "from torch import nn\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Load and run model predictions](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Load the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load('data/model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Model Inference](#toc0_)\n",
    "\n",
    "- Optimizing a models to run on a variety of platforms and programming languages is difficult. It's very time-consuming to maximize performance across all the different combinations of frameworks and hardware. \n",
    "- The Open Neural Network Exchange (ONNX) runtime provides a solution for you to train once and accelerate inference on any hardware, cloud, or edge devices.\n",
    "- ONNX is a common format supported by a number of vendors to share neural networks and other machine learning models. You can use ONNX format to do inference on your model on other programming languages and frameworks such as Java, JavaScript, C# and ML.NET.\n",
    "\n",
    "> **NOTE** ONNX 模型使您能够在不同平台上以不同编程语言运行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Exporting the model to ONNX](#toc0_)\n",
    "\n",
    "- PyTorch also has native ONNX export support.\n",
    "- Given the dynamic nature of the PyTorch execution graph, however, the export process must traverse the execution graph to produce a persisted ONNX model.\n",
    "- For this reason, a test variable of the appropriate size should be passed in to the export routine \n",
    "- (in our case, we will create a dummy zero tensor of the correct size. You can get the size from the shape fuction on your training dataset with tensor.shape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1,28,28))\n",
    "onnx_model = 'data/model.onnx'\n",
    "onnx.export(model, input_image, onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们将使用测试数据集作为示例数据，从 ONNX 模型进行推理以进行预测。\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "print(type(x),type(y))\n",
    "sample_img, _ = test_data[0]\n",
    "sample_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "# 使用 onnxruntime.InferenceSession 创建推理会话。要推断 ONNX 模型，请调用 run 并传入您想要返回的输出列表（如果您需要所有输出，请留空）和输入值的映射\n",
    "session = onnxruntime.InferenceSession(onnx_model, None)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "result = session.run([output_name], {input_name: x.numpy()})\n",
    "predicted, actual = classes[result[0][0].argmax(0)], classes[y]\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
