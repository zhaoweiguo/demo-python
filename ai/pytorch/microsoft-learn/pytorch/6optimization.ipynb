{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Optimizing the model parameters](#toc1_)    \n",
    "  - [Prerequisite code](#toc1_1_)    \n",
    "  - [Setting hyperparameters](#toc1_2_)    \n",
    "  - [Add an optimization loop](#toc1_3_)    \n",
    "    - [Add a loss function](#toc1_3_1_)    \n",
    "    - [Optimization pass](#toc1_3_2_)    \n",
    "  - [Full implementation](#toc1_4_)    \n",
    "  - [Saving Models](#toc1_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Optimizing the model parameters](#toc0_)\n",
    "\n",
    "- Training a model is an iterative process; in each iteration (epoch). The model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous module), and optimizes these parameters using gradient descent.\n",
    "\n",
    "## <a id='toc1_1_'></a>[Prerequisite code](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Setting hyperparameters](#toc0_)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    " - **Number of Epochs** - the number times the entire training dataset is passed through the network. \n",
    " - **Batch Size** - the number of data samples seen by the model in each epoch. Iterates over the number of batches needed to complete an epoch.\n",
    " - **Learning Rate** - the size of steps that the model matches as it searches for the best weights that will produce a higher model accuracy.\n",
    " \n",
    " Smaller values means the model will take a longer time to find the best weights. Larger values may result in the model stepping over and missing the best weights, which yields unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Add an optimization loop](#toc0_)\n",
    "\n",
    "- Each iteration of the optimization loop is called an epoch.\n",
    "- Each epoch consists of two main parts:\n",
    "\n",
    "    - The Train Loop - iterate over the training dataset and try to converge to optimal parameters.\n",
    "    - The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "\n",
    "### <a id='toc1_3_1_'></a>[Add a loss function](#toc0_)\n",
    "\n",
    "常见的损失函数:\n",
    "\n",
    "- nn.MSELoss (Mean Square Error) used for regression tasks\n",
    "- nn.NLLLoss (Negative Log Likelihood) used for classification\n",
    "- nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Optimization pass](#toc0_)\n",
    "\n",
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "    - Call ``optimizer.zero_grad()`` to reset the gradients of model parameters.\n",
    "        默认情况下渐变相加；为了防止重复计算，我们在每次迭代时明确地将它们归零\n",
    "    - Back-propagate the prediction loss with a call to loss.backwards()\n",
    "    - Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Full implementation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300464  [    0/60000]\n",
      "loss: 2.286695  [ 6400/60000]\n",
      "loss: 2.269128  [12800/60000]\n",
      "loss: 2.268317  [19200/60000]\n",
      "loss: 2.258540  [25600/60000]\n",
      "loss: 2.232573  [32000/60000]\n",
      "loss: 2.241280  [38400/60000]\n",
      "loss: 2.215233  [44800/60000]\n",
      "loss: 2.221534  [51200/60000]\n",
      "loss: 2.181303  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 0.034337 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.206467  [    0/60000]\n",
      "loss: 2.196480  [ 6400/60000]\n",
      "loss: 2.132880  [12800/60000]\n",
      "loss: 2.140418  [19200/60000]\n",
      "loss: 2.139766  [25600/60000]\n",
      "loss: 2.075788  [32000/60000]\n",
      "loss: 2.101022  [38400/60000]\n",
      "loss: 2.043405  [44800/60000]\n",
      "loss: 2.072541  [51200/60000]\n",
      "loss: 1.977665  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.031434 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.050360  [    0/60000]\n",
      "loss: 2.031549  [ 6400/60000]\n",
      "loss: 1.892955  [12800/60000]\n",
      "loss: 1.912312  [19200/60000]\n",
      "loss: 1.937670  [25600/60000]\n",
      "loss: 1.818116  [32000/60000]\n",
      "loss: 1.868623  [38400/60000]\n",
      "loss: 1.775286  [44800/60000]\n",
      "loss: 1.861269  [51200/60000]\n",
      "loss: 1.676801  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.027408 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.844311  [    0/60000]\n",
      "loss: 1.819901  [ 6400/60000]\n",
      "loss: 1.606484  [12800/60000]\n",
      "loss: 1.635654  [19200/60000]\n",
      "loss: 1.736201  [25600/60000]\n",
      "loss: 1.569504  [32000/60000]\n",
      "loss: 1.632836  [38400/60000]\n",
      "loss: 1.544010  [44800/60000]\n",
      "loss: 1.658420  [51200/60000]\n",
      "loss: 1.420794  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.023925 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.653101  [    0/60000]\n",
      "loss: 1.637487  [ 6400/60000]\n",
      "loss: 1.371091  [12800/60000]\n",
      "loss: 1.409504  [19200/60000]\n",
      "loss: 1.529896  [25600/60000]\n",
      "loss: 1.390405  [32000/60000]\n",
      "loss: 1.445176  [38400/60000]\n",
      "loss: 1.385701  [44800/60000]\n",
      "loss: 1.485998  [51200/60000]\n",
      "loss: 1.252490  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.021380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.491731  [    0/60000]\n",
      "loss: 1.500868  [ 6400/60000]\n",
      "loss: 1.204958  [12800/60000]\n",
      "loss: 1.261030  [19200/60000]\n",
      "loss: 1.375606  [25600/60000]\n",
      "loss: 1.265365  [32000/60000]\n",
      "loss: 1.317831  [38400/60000]\n",
      "loss: 1.279754  [44800/60000]\n",
      "loss: 1.365017  [51200/60000]\n",
      "loss: 1.148270  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.019669 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.374236  [    0/60000]\n",
      "loss: 1.409233  [ 6400/60000]\n",
      "loss: 1.091955  [12800/60000]\n",
      "loss: 1.164068  [19200/60000]\n",
      "loss: 1.273004  [25600/60000]\n",
      "loss: 1.175588  [32000/60000]\n",
      "loss: 1.230326  [38400/60000]\n",
      "loss: 1.205921  [44800/60000]\n",
      "loss: 1.280463  [51200/60000]\n",
      "loss: 1.077095  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.018477 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.286453  [    0/60000]\n",
      "loss: 1.345556  [ 6400/60000]\n",
      "loss: 1.011179  [12800/60000]\n",
      "loss: 1.097488  [19200/60000]\n",
      "loss: 1.201982  [25600/60000]\n",
      "loss: 1.106424  [32000/60000]\n",
      "loss: 1.166983  [38400/60000]\n",
      "loss: 1.150965  [44800/60000]\n",
      "loss: 1.216513  [51200/60000]\n",
      "loss: 1.024786  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.017583 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.216387  [    0/60000]\n",
      "loss: 1.293315  [ 6400/60000]\n",
      "loss: 0.948442  [12800/60000]\n",
      "loss: 1.044947  [19200/60000]\n",
      "loss: 1.150030  [25600/60000]\n",
      "loss: 1.051222  [32000/60000]\n",
      "loss: 1.118596  [38400/60000]\n",
      "loss: 1.108057  [44800/60000]\n",
      "loss: 1.166193  [51200/60000]\n",
      "loss: 0.984452  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.016884 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.159482  [    0/60000]\n",
      "loss: 1.251839  [ 6400/60000]\n",
      "loss: 0.897295  [12800/60000]\n",
      "loss: 1.002256  [19200/60000]\n",
      "loss: 1.109784  [25600/60000]\n",
      "loss: 1.006674  [32000/60000]\n",
      "loss: 1.080901  [38400/60000]\n",
      "loss: 1.073722  [44800/60000]\n",
      "loss: 1.125458  [51200/60000]\n",
      "loss: 0.951701  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.016322 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Saving Models](#toc0_)\n",
    "\n",
    "- PyTorch 模型将学习到的参数存储在内部状态字典中，称为 state_dict\n",
    "- 这些可以通过 torch.save 方法持久化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
